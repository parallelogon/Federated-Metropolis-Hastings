{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QlsdPP.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "l6MWZHa5r3ch"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QLSD++ "
      ],
      "metadata": {
        "id": "shETNdTmf_gE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOx9YltJcpGL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Written by Zachary Jones\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "# random seed set for reproducability, decreases performance\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set-up and Data preparation"
      ],
      "metadata": {
        "id": "BoJ4SdsNgUmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prior and Preconditioner classes used in gradient calculation"
      ],
      "metadata": {
        "id": "dlJssRa1gG_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Prior():\n",
        "  def __init__(self, model = \"gaussian\", **kwargs):\n",
        "    '''\n",
        "    Indedpendent of the form, each instantation of the 'Prior' class keeps\n",
        "    track of the model used, (gaussian etc.) and checks for parameters.\n",
        "    Each particular instantation of a model must include a 'gradLog' function\n",
        "    which returns the distribution specific gradient of the NEGATIVE log\n",
        "    liklihood.\n",
        "\n",
        "    This is used as part of the gradient calculation for bayesian learning\n",
        "    '''\n",
        "    self.model = model\n",
        "    self.params = kwargs.get(\"params\", None)\n",
        "\n",
        "    if self.params == None:\n",
        "      raise ValueError(\"Something has gone horribly wrong: invalid parameters\")\n",
        "\n",
        "  def __name__(self):\n",
        "    return self.__class__.__name__\n",
        "    \n",
        "  def to(self,device):\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "      \n",
        "class Gaussian(Prior):\n",
        "  def __init__(self, mean = 0.0, sig = 0.1):\n",
        "    super(Gaussian, self).__init__(\"gaussian\", params = {\"mean\": mean, \"sig\": sig})\n",
        "    self.mean = torch.tensor([mean])\n",
        "    self.sig = torch.tensor([sig])\n",
        "\n",
        "  def prior(self, theta):\n",
        "\n",
        "    logPrior = torch.norm(theta - self.mean)\n",
        "    logPrior=-logPrior*logPrior/(2*self.sig**2)\\\n",
        "    -.5*torch.log(2*torch.pi*self.sig**2)*len(theta)\n",
        "\n",
        "    gradLogPrior = (theta - self.mean)/self.sig**2\n",
        "\n",
        "    self.logPrior = logPrior\n",
        "    self.gradLogPrior = gradLogPrior\n",
        "\n",
        "    return(logPrior, gradLogPrior)\n",
        "\n",
        "\n",
        "  # optimize the negative log liklihood later in the model, so this is the grad\n",
        "  # of the negative log prior\n",
        "  def grad_log(self, theta):\n",
        "    return (theta - self.mean)/self.sig**2\n",
        "\n",
        "  def to(self,device):\n",
        "    self.device = device\n",
        "    self.mean = self.mean.to(device)\n",
        "    self.sig = self.sig.to(device)\n",
        "\n",
        "\n",
        "# Assuming normally distributed weights this is the conjugate prior.\n",
        "class InverseGamma(Prior):\n",
        "  def __init__(self, alpha = 0.01, beta = 0.01):\n",
        "    super(InverseGamma, self).__init__(\"inv_gamma\", params = {\"alpha\" : alpha,\n",
        "                                                              \"beta\": beta})\n",
        "    self.alpha = alpha\n",
        "    self.beta = beta\n",
        "\n",
        "  def __name__(self):\n",
        "    return \"InverseGamma\"\n",
        "\n",
        "  def prior(self, theta):\n",
        "    logPrior = self.alpha * torch.log(self.beta) - (self.alpha + 1) \\\n",
        "    * torch.log(theta) - self.beta/theta - torch.lgamma(self.alpha)\n",
        "\n",
        "    gradLogPrior = -(self.alpha + 1)/theta + self.beta/theta**2\n",
        "    return(logPrior, gradLogPrior)\n",
        "    \n",
        "  def grad_log(self, theta):\n",
        "    gradLogPrior = -(self.alpha + 1)/theta + self.beta/theta**2\n",
        "    return gradLogPrior\n",
        "\n",
        "  def to(self,device):\n",
        "    self.device = device\n",
        "    self.alpha = torch.tensor([self.alpha]).to(device)\n",
        "    self.beta = torch.tensor([self.beta]).to(device)"
      ],
      "metadata": {
        "id": "suTmCIb3cv-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from scipy.linalg import fractional_matrix_power\n",
        "\n",
        "class Preconditioner():\n",
        "  def __init__(self, d1, d2):\n",
        "    '''\n",
        "    As the preconditioner must operate on both weights and biases in the model\n",
        "    according to their dimension, which are unknown before hand, they are set\n",
        "    at instantation.  Using 'torch.Einsum('ij, i...', A, sample)', allows for\n",
        "    both speed and flexibility in the form of the inner product (biases are kept\n",
        "    by pytorch as 1d tensors so matmul operations cannot be used)\n",
        "\n",
        "    Each 'Preconditioner' must have an 'Apply' and 'Sample' function, which are\n",
        "    used at different stages of gradient calculation.\n",
        "    \n",
        "    Apply : return an (N_out x N_out matrix) with N_out beind the output\n",
        "    dimension of the layer\n",
        "\n",
        "    Sample: Return an N_out x N_out matrix which has been multiplied by a matrix\n",
        "    of iid random normal samples with dimension (N_out x N_in) (the same as the\n",
        "    weights)\n",
        "    '''\n",
        "    if d2:\n",
        "      self.type = 'weight'\n",
        "      self.input_size = d2\n",
        "\n",
        "    else:\n",
        "      self.input_size = 1\n",
        "      self.type = \"bias\"\n",
        "\n",
        "    self.output_size = d1\n",
        "    self.C = torch.eye(d1)\n",
        "    self.A = torch.eye(d1)\n",
        "\n",
        "\n",
        "  def update(self, grad):\n",
        "    pass\n",
        "\n",
        "  def to(self, device):\n",
        "    self.C = self.C.to(device)\n",
        "    self.A = self.A.to(device)\n",
        "    self.device = device\n",
        "    return self\n",
        "\n",
        "# Euclidean preconditioner for use in gradient step does not change the\n",
        "# distribution of the weights.  \n",
        "class Euclidean(Preconditioner):\n",
        "  def __init__(self, d1, d2 = None):\n",
        "    super(Euclidean, self).__init__(d1, d2)\n",
        "\n",
        "  def apply(self):\n",
        "    return self.C\n",
        "\n",
        "  def sample(self):\n",
        "    if self.type == 'weight':\n",
        "      sample = torch.randn(self.output_size, self.input_size).to(self.device)\n",
        "    else:\n",
        "      sample = torch.randn(self.output_size).to(self.device)\n",
        "\n",
        "    # use torch.einsum instead of distinguishing between bias and weight terms,\n",
        "    # not particularly slow.\n",
        "    return(torch.einsum(\"ij,j... -> i...\",self.A, sample))\n",
        "\n",
        "\n",
        "# RMSProp preconditioner corresponds to giving each weight a seperate learning\n",
        "# rate\n",
        "class RMSProp(Preconditioner):\n",
        "  def __init__(self, d1, d2 = None, decay_rate = .5, regularizer = 1e-7):\n",
        "    super(RMSProp, self).__init__(d1, d2)\n",
        "    self.decay_rate = decay_rate\n",
        "    self.regularizer = regularizer\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update(self,grad):\n",
        "    if self.type == \"weight\":\n",
        "      g_sq = torch.einsum(\"ij,ij->i\",grad,grad).diag()\n",
        "\n",
        "    else:\n",
        "      g_sq = torch.mul(grad,grad)\n",
        "\n",
        "    self.C.mul_(1.0 - self.decay_rate).add_(g_sq, alpha = self.decay_rate)\n",
        "\n",
        "\n",
        "    local_time = 1./(self.decay_rate**2) + 1.0\n",
        "    self.decay_rate = 1.0/sqrt(local_time)\n",
        "\n",
        "  def apply(self):\n",
        "    c = ((self.C.diag() + self.regularizer).pow(-0.5)).diag()\n",
        "    return c\n",
        "\n",
        "  def sample(self):\n",
        "    if self.type == 'weight':\n",
        "      sample = torch.randn(self.output_size, self.input_size).to(self.device)\n",
        "    else:\n",
        "      sample = torch.randn(self.output_size).to(self.device)\n",
        "\n",
        "    c = ((self.C.diag() + self.regularizer).pow(-0.25)).diag()\n",
        "    return(torch.einsum(\"ij,j... -> i...\",c , sample))\n",
        "\n",
        "  def to(self, device):\n",
        "    self.A = self.A.to(device)\n",
        "    self.C = self.C.to(device)\n",
        "    self.device = device\n",
        "    self.regularizer = torch.tensor([self.regularizer]).to(device)\n",
        "    return(self)\n",
        "\n",
        "\n",
        "# Adam preconditioner corresponds to giving each weight a seperate learning\n",
        "# rate\n",
        "class Adam(Preconditioner):\n",
        "  def __init__(self, d1, d2 = None, decay_rate = .5, regularizer = 1e-7):\n",
        "    super(Adam, self).__init__(d1, d2)\n",
        "    self.decay_rate = decay_rate\n",
        "    self.regularizer = regularizer\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update(self,grad):\n",
        "    if self.type == \"weight\":\n",
        "      g_sq = torch.einsum(\"ij,ij->i\",grad,grad).diag()\n",
        "\n",
        "    else:\n",
        "      g_sq = torch.mul(grad,grad)\n",
        "\n",
        "    g = g_sq.sqrt()\n",
        "\n",
        "    self.C.mul_(1.0 - self.decay_rate).add_(g_sq, alpha = self.decay_rate)\n",
        "    self.A.mul_(1.0 - .9/.99*self.decay_rate).add_(g, alpha = .9/.99*self.decay_rate)\n",
        "\n",
        "\n",
        "    local_time = 1./(self.decay_rate**2) + 1.0\n",
        "    self.decay_rate = 1.0/sqrt(local_time)\n",
        "\n",
        "  def apply(self):\n",
        "    c = (self.A.diag()*((self.C.diag() + self.regularizer).pow(-0.5))).diag()\n",
        "    return c\n",
        "\n",
        "  def sample(self):\n",
        "    if self.type == 'weight':\n",
        "      sample = torch.randn(self.output_size, self.input_size).to(self.device)\n",
        "    else:\n",
        "      sample = torch.randn(self.output_size).to(self.device)\n",
        "\n",
        "    c = (self.A.diag().pow(.5)*((self.C.diag() + self.regularizer).pow(-0.25))).diag()\n",
        "    return(torch.einsum(\"ij,j... -> i...\",c , sample))\n",
        "\n",
        "  def to(self, device):\n",
        "    self.A = self.A.to(device)\n",
        "    self.C = self.C.to(device)\n",
        "    self.device = device\n",
        "    self.regularizer = torch.tensor([self.regularizer]).to(device)\n",
        "    return(self)\n",
        "\n",
        "\n",
        "# inverting the fisher information matrix is extremely espensive, restrict to\n",
        "# smaller models.  Fisher information gives the curvature around the gradient\n",
        "# estimator and yields Amari's natural gradient\n",
        "class Fisher(Preconditioner):\n",
        "  def __init__(self, d1, d2 = None, decay_rate = .1, regularizer = 1e-7):\n",
        "    super(Fisher, self).__init__(d1, d2)\n",
        "    self.decay_rate = decay_rate\n",
        "    self.regularizer = regularizer\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update(self, grad):\n",
        "    self.C.mul_(1.0 - self.decay_rate)\n",
        "\n",
        "    if self.type != 'weight':\n",
        "      grad = grad.unsqueeze(-1)\n",
        "\n",
        "    self.C.add_(torch.matmul(grad, grad.T),alpha = self.decay_rate)\n",
        "\n",
        "    local_time = 1.0/self.decay_rate + 1.0\n",
        "    self.decay_rate = 1.0/sqrt(local_time)\n",
        "\n",
        "  def apply(self):\n",
        "    return torch.linalg.inv(torch.add(self.C,\n",
        "                                      torch.eye(self.output_size).to(self.device),\n",
        "                                      alpha = self.regularizer))\n",
        "\n",
        "  def sample(self):\n",
        "    if self.type == \"weight\":\n",
        "      sample = torch.randn(self.output_size, self.input_size).to(self.device)\n",
        "    else:\n",
        "      sample = torch.randn(self.output_size).to(self.device)\n",
        "    \n",
        "    pre = torch.add(self.C, torch.eye(self.output_size).to(device), \\\n",
        "                    alpha = self.regularizer)\n",
        "    \n",
        "    # find a better matrix square root algo this is slow and cumbersome\n",
        "    return torch.linalg.solve(\n",
        "        torch.from_numpy(\n",
        "            fractional_matrix_power(\n",
        "                pre.cpu(), 0.5)).to(torch.float).to(self.device), sample)\n",
        "\n",
        "  def to(self, device):\n",
        "    self.device = device\n",
        "    self.A = self.A.to(device)\n",
        "    self.C = self.C.to(device)\n",
        "    return(self)\n",
        "\n",
        "# The quasi diagonal cholesky decomposition of the inverse fisher information \n",
        "# agrees with the inverse fisher information matrix on the diagonals and the \n",
        "# first row.  Use this as an approximation of the fisher info for larger models.\n",
        "class QDC(Preconditioner):\n",
        "  def __init__(self, d1, d2 = None, decay_rate = .5, regularizer = 1e-6):\n",
        "    super(QDC,self).__init__(d1,d2)\n",
        "    self.decay_rate = decay_rate\n",
        "    self.regularizer = regularizer\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def update(self, grad):\n",
        "    self.C.mul_(1.0 - self.decay_rate)\n",
        "\n",
        "\n",
        "    '''\n",
        "    if self.type == \"weight\":\n",
        "      g_row = torch.matmul(grad[0,:], grad.T)\n",
        "      g_diag = torch.einsum(\"ij,ji -> i\", grad, grad.T).diag()\n",
        "\n",
        "    else:\n",
        "      grad_unsqueezed = torch.unsqueeze(grad,-1)\n",
        "      g_row = torch.mul(grad[0], grad_unsqueezed.T)\n",
        "      g_diag = torch.mul(grad, grad).diag()\n",
        "    '''\n",
        "\n",
        "    if self.type != 'weight':\n",
        "      grad = grad.unsqueeze(-1)\n",
        "\n",
        "    g = torch.matmul(grad, grad.T)\n",
        "    g_diag = g.diag()\n",
        "    g_row = g[0,:]\n",
        "\n",
        "    g_factor = g_diag.diag()\n",
        "    g_factor[0,:] = g_row\n",
        "\n",
        "    self.C.add_(g_factor, alpha = self.decay_rate)\n",
        "\n",
        "    self.A = torch.zeros_like(self.A)\n",
        "\n",
        "    a00 = torch.add(self.C[0,0], self.regularizer).pow(-0.5)\n",
        "    self.A[0,0] = a00\n",
        "\n",
        "    if a00.isnan():\n",
        "      print(a00)\n",
        "\n",
        "    for i in range(1,len(self.A)):\n",
        "      a00C0i = torch.mul(a00,self.C[0,i]).pow(2)\n",
        "\n",
        "\n",
        "      if a00C0i.isnan():\n",
        "        print(a00, self.C[0,i])\n",
        "        raise ValueError('nan')\n",
        "\n",
        "      Ciia00C0i = torch.add(self.C[i,i], a00C0i, alpha = -1.0)\n",
        "\n",
        "      \n",
        "      if Ciia00C0i.isnan():\n",
        "        print(self.C[i,i], a00C0i)\n",
        "        raise ValueError('nan')\n",
        "\n",
        "      self.A[i,i] = torch.pow(Ciia00C0i + self.regularizer, -0.5)\n",
        "\n",
        "      AiiC0i = self.A[i,i].mul(self.C[0,i])\n",
        "\n",
        "      \n",
        "      if AiiC0i.isnan():\n",
        "        print(self.A[i,i], self.C[0,i])\n",
        "        raise ValueError('nan')\n",
        "\n",
        "      self.A[0,i] = torch.mul(-(a00.pow(2)), AiiC0i)\n",
        "\n",
        "    local_time = 1.0/self.decay_rate + 1.0\n",
        "    self.decay_rate = 1.0/sqrt(local_time)\n",
        "\n",
        "  def apply(self):\n",
        "    if self.A.isnan().any():\n",
        "      print(self.A)\n",
        "      raise ValueError('dicks')\n",
        "\n",
        "    return self.A.matmul(self.A.T)\n",
        "\n",
        "  def sample(self):\n",
        "    if self.type == \"weight\":\n",
        "      sample = torch.randn(self.output_size, self.input_size).to(self.device)\n",
        "    else:\n",
        "      sample = torch.randn(self.output_size).to(self.device)\n",
        "\n",
        "    return(torch.einsum(\"ij,j... -> i...\", self.A, sample))\n",
        "\n",
        "  def to(self, device):\n",
        "    self.device = device\n",
        "    self.A = self.A.to(device)\n",
        "    self.C = self.C.to(device)\n",
        "    return(self)"
      ],
      "metadata": {
        "id": "Vos-MoJedJ2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset: MNIST"
      ],
      "metadata": {
        "id": "0RupSZ3sf6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST = torchvision.datasets.MNIST(root = \".\", \n",
        "                                   train = True,\n",
        "                                   download = True)\n",
        "\n",
        "output_dimension = len(MNIST.classes)\n",
        "input_dimension = len(MNIST.data[0])**2\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "dxRJlQ6Ugicb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST.data[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlxSUVeJkxWm",
        "outputId": "da3e4978-4238-47c7-e374-393f09600814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST.targets[0]"
      ],
      "metadata": {
        "id": "FTCaB9D3k6Uu",
        "outputId": "eb1fc528-5a0f-4eba-8e4f-d6cfd848321e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Structure\n",
        "\n",
        "A thoughtful datastructure simpliefies the minimization, since the whole gradient must be \n",
        "\n",
        "calculated periodically and individual gradient samples must be matched in a variance\n",
        "\n",
        "reduction technique."
      ],
      "metadata": {
        "id": "VbRu93TAgnE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleSplitData(torch.utils.data.Dataset):\n",
        "  def __init__(self, dataset, k, transform = None):\n",
        "    '''\n",
        "    The dataset given to each virtual submodel is kept in a list of k tuples\n",
        "    corresponding to the local dataset each with a data tensor of dimension \n",
        "    (N_k, d), their corresponding labels, and the index of the data point.\n",
        "    This allows for the index of points used to be passed to the model for\n",
        "    individual gradient evaluations.\n",
        "\n",
        "    Images are accessed as handle[i, j][0] with image j of partition i\n",
        "    Their corresponding target as handle [i, j][1]\n",
        "    The index as [i, j][2]\n",
        "    '''\n",
        "\n",
        "    N = len(dataset)\n",
        "\n",
        "    splits = np.random.choice(k, N, replace = True)\n",
        "    partition_sizes = [np.sum(splits == i) for i in range(k)]\n",
        "    indices = [torch.arange(partition_sizes[i]) for i in range(k)]\n",
        "\n",
        "    partition_sizes = [0] + partition_sizes\n",
        "    partition_boundaries = np.cumsum(partition_sizes)\n",
        "\n",
        "\n",
        "    if transform is None:\n",
        "      split_data = [dataset.data[partition_boundaries[i]:partition_boundaries[i+1],:,:]\\\n",
        "                    for i in range(len(partition_boundaries)-1)]\n",
        "    else:\n",
        "      split_data = [dataset.data[partition_boundaries[i]:partition_boundaries[i+1],:,:]\\\n",
        "                    for i in range(len(partition_boundaries)-1)]   \n",
        "\n",
        "    split_targets = [dataset.targets[partition_boundaries[i]:partition_boundaries[i+1]]\\\n",
        "                  for i in range(len(partition_boundaries)-1)]\n",
        "\n",
        "    self.N = max(partition_sizes)\n",
        "\n",
        "    self.k = k\n",
        "\n",
        "    self.data = split_data\n",
        "\n",
        "    self.targets = split_targets\n",
        "\n",
        "    self.indices = indices\n",
        "\n",
        "    self.partition_sizes = partition_sizes[1:]\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    if self.transform:\n",
        "      data = self.transform(self.data[idx[0]][idx[1],:,:])\n",
        "\n",
        "    else:\n",
        "      data = self.data[idx[0]][idx[1],:,:].to(torch.float)\n",
        "\n",
        "    return (data,\n",
        "            self.targets[idx[0]][idx[1]],\n",
        "            self.indices[idx[0]][idx[1]])\n",
        "\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.N\n",
        "\n",
        "  def shuffle(self):\n",
        "    for i in range(self.k):\n",
        "      n = len(self.indices[i])\n",
        "      perm = np.random.choice(n,n, replace = False)\n",
        "\n",
        "      self.data[i] = self.data[i][perm,:,:]\n",
        "      self.targets[i] = self.targets[i][perm]\n",
        "      self.indices[i] = torch.from_numpy(perm)\n",
        "\n",
        "  def sample(self, N = None):\n",
        "    if N == None:\n",
        "\n",
        "      if self.transform:\n",
        "        data =[self.transform(data) for data in self.data]\n",
        "      else:\n",
        "        data = [data.to(torch.float) for data in self.data]\n",
        "        \n",
        "      return (data,\n",
        "              self.targets,\n",
        "              self.indices)\n",
        "\n",
        "    else:\n",
        "      data = []\n",
        "      targets = []\n",
        "      indices = []\n",
        "\n",
        "      for i in range(self.k):\n",
        "        n = len(self.indices[i])\n",
        "        perm = np.random.choice(n, N)\n",
        "\n",
        "        if self.transform:\n",
        "          data_i = self.transform(self.data[i][perm, :, :])\n",
        "        else:\n",
        "          data_i = self.data[i][perm, :, :].to(torch.float)\n",
        "          \n",
        "        data.append(data_i)\n",
        "        targets.append(self.targets[i][perm])\n",
        "        indices.append(self.indices[i][perm])\n",
        "\n",
        "      return(data, targets, indices)\n"
      ],
      "metadata": {
        "id": "b28q0RDNgtGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Per-sample gradients: Use Opacus Library"
      ],
      "metadata": {
        "id": "t_VAn1GHqkGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opacus\n",
        "from opacus.grad_sample import GradSampleModule"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK5F9GeVqZ25",
        "outputId": "53f85478-3f0c-45d0-bddf-becc40304c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opacus in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.11.0+cu113)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from opacus) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8->opacus) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Compression"
      ],
      "metadata": {
        "id": "Sd08Mc55qqOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize(v, s):\n",
        "\n",
        "  # Different operations required for different gradient dimensions (weights vs\n",
        "  # biases)\n",
        "  \n",
        "  if len(v.shape) > 1:\n",
        "    v_norm = v.norm(dim = 1)\n",
        "    \n",
        "    l = torch.einsum(\"ij, i-> ij\", v.abs().mul(s), v_norm.pow(-1))\n",
        "    deterministic_part = torch.einsum(\"ij,i-> ij\", v.sign(), v_norm).div(s)\n",
        "\n",
        "  else:\n",
        "    v_norm = v.norm()\n",
        "\n",
        "    l = v.abs().mul(s).div(v_norm)\n",
        "    deterministic_part = v_norm.mul(v.sign()).div(s)\n",
        "\n",
        "  random_part = l.floor().add(torch.rand_like(v).le(torch.add(l, l.floor(), alpha = -1)).mul(1.0))\n",
        "  out = deterministic_part.mul(random_part)\n",
        "  \n",
        "  # If the gradient is zero in some dimensions it will result in NaNs due to the\n",
        "  # normalization above. These points should be kept as Zeros.\n",
        "  out[out.isnan()] = 0.\n",
        "  \n",
        "  return(out)"
      ],
      "metadata": {
        "id": "W7hfO2w_quI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer objects\n",
        "\n",
        "In order to streamline training, the QLSD++ algorithm is split into two parts\n",
        "an optimizer object meant to load onto local agents and a counterpart for the\n",
        "central server."
      ],
      "metadata": {
        "id": "IY6e9rTlrOwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "-Investigate this warning\n",
        "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1025: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
      ],
      "metadata": {
        "id": "DrvHBm-urMBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local QLSD Optimizer"
      ],
      "metadata": {
        "id": "l6MWZHa5r3ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch._C import parse_schema\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import math\n",
        "\n",
        "class QlsdPPLocal(Optimizer):\n",
        "  '''\n",
        "  Local model which does not modify parameters during a \"step\" phase, only\n",
        "  modifying the gradients of the parameters to be aggregated later in a\n",
        "  step of the central model.  Each distributed model will has its own associated\n",
        "  optimizer which manages layerwise operations and gradient calculations.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, params,\n",
        "               prior = Gaussian(0,.1), \n",
        "               N = required,\n",
        "               l = required,\n",
        "               s = required):\n",
        "\n",
        "    if not prior:\n",
        "      raise ValueError(\"No prior specified\")\n",
        "\n",
        "    if N is not required and N < 0:\n",
        "      raise ValueError(\"Need the size of the local dataset\")\n",
        "\n",
        "    if l is not required and (l < 0 or l % 1 != 0):\n",
        "      raise ValueError(\"Control variable periodicity must be a nonnegative integer\")\n",
        "\n",
        "    if s is not required and (s < 0 or s % 1 != 0):\n",
        "      raise ValueError('Quantization must be a nonnegative integer')\n",
        "\n",
        "    defaults = dict(prior = prior,\n",
        "                    N = N,\n",
        "                    eta = [],\n",
        "                    alpha = [],\n",
        "                    Uzeta = [],\n",
        "                    Uzeta_ij = [],\n",
        "                    l = l,\n",
        "                    s = s)\n",
        "    \n",
        "    super(QlsdPPLocal, self).__init__(params, defaults)\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      # Memory term used in quantization to reduce error\n",
        "      eta = []\n",
        "\n",
        "      # memory term learning rate\n",
        "      # alpha should be initialized as 1/(omega + 1) as in the paper\n",
        "      # omega = min(min(d/s2, √d/s) with d the dimension of the gradient\n",
        "      alpha = []\n",
        "\n",
        "      # initialize all these values after p has been stored as a parameter\n",
        "      with torch.no_grad():\n",
        "        for p in group[\"params\"]:\n",
        "          eta.append(torch.zeros_like(p).to(p.device))\n",
        "          d = p.shape[-1]\n",
        "          alpha.append(1/(1 + min(d/(s**2), math.sqrt(d)/s)))\n",
        "\n",
        "        group['eta'] = eta\n",
        "        group['alpha'] = alpha\n",
        "\n",
        "        # stores the individual samples for the whole gradient, has a shape\n",
        "        # B x g where B is the batch size and g is the dimensions of the \n",
        "        # gradient tensor\n",
        "        group['Uzeta_ij'] = []\n",
        "        group['Uzeta'] = []\n",
        "\n",
        "    self.param_groups[0][\"prior\"].to(self.param_groups[0][\"params\"][0].device)\n",
        "\n",
        "  def __setstate__(self, state):\n",
        "    super(QlsdPPLocal, self).__setstate__(state)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, t, indices, closure=None):\n",
        "\n",
        "    # legacy code, not used but could be\n",
        "    loss = None\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      prior = group['prior']\n",
        "      N = group['N']\n",
        "      s = group['s']\n",
        "      eta = group['eta']\n",
        "      Uzeta = group['Uzeta']\n",
        "      alpha = group['alpha']\n",
        "      Uzeta_ij = group['Uzeta_ij']\n",
        "      l = group['l']\n",
        "\n",
        "      for i, p in enumerate(group['params']):\n",
        "\n",
        "        if p.grad_sample is not None:\n",
        "          if p.grad is not None:\n",
        "            if t % l == 0:\n",
        "              d_p_full = p.grad_sample.add(prior.grad_log(p), alpha = 1.0/N)\n",
        "              d_p = d_p_full.sum(dim = 0)\n",
        "\n",
        "              if t == 0:\n",
        "                Uzeta_ij.append(d_p_full)\n",
        "                Uzeta.append(d_p)\n",
        "\n",
        "              # if t % l = 0 and t != 0 one doesn't have to build the grad\n",
        "              else:\n",
        "                Uzeta_ij[i] = d_p_full\n",
        "                Uzeta[i] = d_p\n",
        "\n",
        "            else: \n",
        "              assert len(indices) <= len(Uzeta_ij[i]), \"\"\"Cannot index over the \n",
        "              number of samples\"\"\"\n",
        "\n",
        "              p.grad_sample.add_(prior.grad_log(p), alpha = 1.0/N)\n",
        "\n",
        "              g_diff = torch.mean(\n",
        "                  p.grad_sample.add(Uzeta_ij[i][indices,...], alpha = -1.0), dim = 0)\n",
        "              \n",
        "              d_p = torch.add(Uzeta[i], g_diff, alpha = 1.0*N)\n",
        "\n",
        "            d_p.add_(eta[i], alpha = -1.0)\n",
        "\n",
        "            if s is not None:\n",
        "              p.grad = quantize(d_p, s)\n",
        "            else:\n",
        "              p.grad = d_p\n",
        "\n",
        "            # test code, check if gradient minimizes loss function, uncomment\n",
        "            # along with below to see\n",
        "            # p.add_(p.grad, alpha = -.005)\n",
        "\n",
        "            eta[i].add_(p.grad, alpha = alpha[i])\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "BJMq_-nXrkNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "x1 = torch.randn(50,2)\n",
        "x2 = torch.randn(50,2)+ 100\n",
        "x = torch.vstack((x1, x2))\n",
        "x = (x - x.mean(dim = 0))/x.std(dim = 0)\n",
        "y = torch.vstack((torch.ones(x1.shape[0],1), torch.zeros(x2.shape[0],1)))\n",
        "perm = np.random.choice(100, 100, replace = False)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "X = x[perm,:]\n",
        "Y = y[perm,:].squeeze()\n",
        "\n",
        "indices = torch.arange(100)\n",
        "loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "fc = nn.Linear(2,2)\n",
        "fc = GradSampleModule(fc)\n",
        "optimizer = QlsdPPLocal(fc.parameters(),\n",
        "                        prior = Gaussian(0,0.1),\n",
        "                        N = 100,\n",
        "                        l = 20,\n",
        "                        s = 1)\n",
        "l = 20\n",
        "weight_grads = []\n",
        "bias_grads = []\n",
        "training_loss = []\n",
        "for t in range(2000):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if t % l == 0:\n",
        "    x = X\n",
        "    y = Y\n",
        "    sample = indices\n",
        "  else:\n",
        "    sample = np.random.choice(100, 20, replace = False)\n",
        "    x = X[sample,:]\n",
        "    y = Y[sample]\n",
        "\n",
        "  fc.zero_grad()\n",
        "  y_hat = fc(x)\n",
        "  if y_hat.isnan().any():\n",
        "    print('Nan')\n",
        "    break\n",
        "  loss = loss_criterion(y_hat, y.to(torch.long))\n",
        "  loss.backward()\n",
        "  optimizer.step(t, sample)\n",
        "  for name, param in fc.named_parameters():\n",
        "    if \"weight\" in name:\n",
        "      weight_grads.append(param.grad.norm())\n",
        "    else:\n",
        "      bias_grads.append(param.grad.norm())\n",
        "  training_loss.append(loss.item())\n",
        "  if t % 50 == 0:\n",
        "    print(t, loss.item())\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "erMTLdkLrmy2",
        "outputId": "b9f4dbf5-d659-4f6a-cdf6-9285bba21579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nx1 = torch.randn(50,2)\\nx2 = torch.randn(50,2)+ 100\\nx = torch.vstack((x1, x2))\\nx = (x - x.mean(dim = 0))/x.std(dim = 0)\\ny = torch.vstack((torch.ones(x1.shape[0],1), torch.zeros(x2.shape[0],1)))\\nperm = np.random.choice(100, 100, replace = False)\\nprint(x.shape)\\nprint(y.shape)\\nX = x[perm,:]\\nY = y[perm,:].squeeze()\\n\\nindices = torch.arange(100)\\nloss_criterion = torch.nn.CrossEntropyLoss()\\nfc = nn.Linear(2,2)\\nfc = GradSampleModule(fc)\\noptimizer = QlsdPPLocal(fc.parameters(),\\n                        prior = Gaussian(0,0.1),\\n                        N = 100,\\n                        l = 20,\\n                        s = 1)\\nl = 20\\nweight_grads = []\\nbias_grads = []\\ntraining_loss = []\\nfor t in range(2000):\\n  optimizer.zero_grad()\\n\\n  if t % l == 0:\\n    x = X\\n    y = Y\\n    sample = indices\\n  else:\\n    sample = np.random.choice(100, 20, replace = False)\\n    x = X[sample,:]\\n    y = Y[sample]\\n\\n  fc.zero_grad()\\n  y_hat = fc(x)\\n  if y_hat.isnan().any():\\n    print(\\'Nan\\')\\n    break\\n  loss = loss_criterion(y_hat, y.to(torch.long))\\n  loss.backward()\\n  optimizer.step(t, sample)\\n  for name, param in fc.named_parameters():\\n    if \"weight\" in name:\\n      weight_grads.append(param.grad.norm())\\n    else:\\n      bias_grads.append(param.grad.norm())\\n  training_loss.append(loss.item())\\n  if t % 50 == 0:\\n    print(t, loss.item())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "plt.subplot(1,3,1)\n",
        "plt.title('Weight Gradient Norms')\n",
        "plt.plot(weight_grads)\n",
        "plt.subplot(1,3,2)\n",
        "plt.title('Bias Gradient Norms')\n",
        "plt.plot(bias_grads)\n",
        "plt.subplot(1,3,3)\n",
        "plt.title('Training Loss')\n",
        "plt.plot(training_loss)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "AmzDoelFrqAa",
        "outputId": "e6215005-6552-4dad-ea3f-7b23da2a7868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nplt.subplot(1,3,1)\\nplt.title('Weight Gradient Norms')\\nplt.plot(weight_grads)\\nplt.subplot(1,3,2)\\nplt.title('Bias Gradient Norms')\\nplt.plot(bias_grads)\\nplt.subplot(1,3,3)\\nplt.title('Training Loss')\\nplt.plot(training_loss)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Central QLSD Optimizer"
      ],
      "metadata": {
        "id": "ui7AZ4WDrqJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QlsdPPCentral(Optimizer):\n",
        "\n",
        "  '''\n",
        "  Central Optimizer, the step() method takes a list of gradients and combines\n",
        "  them as should be done on the central server in the QLSD++ algorithm as\n",
        "  described in https://arxiv.org/abs/2106.00797\n",
        "  '''\n",
        "  def __init__(self, params, \n",
        "               lr=required,\n",
        "               b = required,\n",
        "               s = required,\n",
        "               preconditioner = required,\n",
        "               MALA = False):\n",
        "\n",
        "    if lr is not required and lr < 0.0:\n",
        "      raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "\n",
        "    if preconditioner is not required and preconditioner.__name__ not in \\\n",
        "    [\"Euclidean\",\"RMSProp\",\"Fisher\",\"QDC\",\"Adam\"]:\n",
        "      raise ValueError(\"Invalid choice of preconditioner: {}\".format(preconditioner))\n",
        "    else:\n",
        "      preconditioner = preconditioner\n",
        "\n",
        "    if b is not required and (b <= 0 or b% 1 != 0):\n",
        "      raise ValueError(\"Number of workers must be a positive integer\")\n",
        "\n",
        "    if s is not required and (s < 0 or s % 1 != 0):\n",
        "      raise ValueError('Quantization must be a nonnegative integer')\n",
        "\n",
        "    defaults = dict(lr = lr,\n",
        "                    preconditioner = preconditioner, \n",
        "                    alpha = [],\n",
        "                    eta = [],\n",
        "                    b = b,\n",
        "                    MALA = MALA)\n",
        "    \n",
        "    super(QlsdPPCentral, self).__init__(params, defaults)\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      p_group = []\n",
        "      eta = []\n",
        "      alpha = []\n",
        "      for p in group[\"params\"]:\n",
        "        p_group.append(preconditioner(*p.shape).to(p.device))\n",
        "        d = p.shape[-1]\n",
        "        eta.append(torch.zeros_like(p).to(p.device))\n",
        "        alpha.append(1/(1 + min(d/(s**2), math.sqrt(d)/s)))\n",
        "\n",
        "      group['preconditioner'] = p_group  \n",
        "      group['eta'] = eta\n",
        "      group['alpha'] = alpha\n",
        "\n",
        "  def __setstate__(self, state):\n",
        "    super(QlsdPPCentral, self).__setstate__(state)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self, grads, closure=None):\n",
        "\n",
        "    loss = None\n",
        "    if closure is not None:\n",
        "      with torch.enable_grad():\n",
        "        loss = closure()\n",
        "\n",
        "    for group in self.param_groups:\n",
        "      lr = group['lr']\n",
        "      preconditioner = group['preconditioner']\n",
        "      eta = group['eta']\n",
        "      alpha = group['alpha']\n",
        "      b = group['b']\n",
        "      MALA = group[\"MALA\"]\n",
        "\n",
        "      # SGD update modified\n",
        "      for i,p in enumerate(group['params']):\n",
        "\n",
        "        a = len(grads)\n",
        "        if a > 0:\n",
        "          grad = sum(grads[j][i] for j in range(len(grads)))\n",
        "          \n",
        "          p.grad = torch.add(eta[i], grad, alpha = 1.0*b/a)\n",
        "\n",
        "          eta[i].add_(grad, alpha = alpha[i])\n",
        "\n",
        "          preconditioner[i].update(p.grad)\n",
        "\n",
        "          d_p = torch.einsum(\"ij,j...-> i...\", preconditioner[i].apply(), p.grad)\n",
        "\n",
        "\n",
        "          p_proposed = p.add(d_p, alpha = -lr)\n",
        "          #p.add_(d_p, alpha = -lr)\n",
        "\n",
        "          noise = preconditioner[i].sample()\n",
        "          \n",
        "          p_proposed.add_(noise, alpha = sqrt(2.0*lr))\n",
        "          #p.add_(noise, alpha = sqrt(2.0*lr))\n",
        "\n",
        "          if not MALA:\n",
        "            p.data = p_proposed.data\n",
        "\n",
        "          else:\n",
        "\n",
        "            dtheta = p_proposed - p\n",
        "\n",
        "            dl_dtheta = -d_p.mul(dtheta)\n",
        "            A = 1 + dl_dtheta\n",
        "\n",
        "            if len(d_p.shape) == 1:\n",
        "              d_p = d_p.unsqueeze(-1)\n",
        "\n",
        "            aa = d_p.matmul(d_p.T).matmul(noise)\n",
        "            bb = noise.mul(aa)\n",
        "            B = 0.5*(dl_dtheta.add(bb, alpha = -lr))\n",
        "            A.mul_(B.exp())\n",
        "\n",
        "            A = torch.minimum(torch.ones_like(A),A)\n",
        "\n",
        "            u = torch.rand_like(A)\n",
        "\n",
        "            selector = u <= A\n",
        "            accepted = torch.where(selector)\n",
        "            rate = torch.mean(1.0*selector)\n",
        "\n",
        "            # Experimental, adjusting learning rate for optimal sampling\n",
        "            # works with ADAM and Euclidean preconditioned dynamics, is\n",
        "            # unstable with RMSProp\n",
        "            \n",
        "            if rate <= 0.55:\n",
        "              group['lr'] *= .99\n",
        "            elif rate >=.6:\n",
        "              group['lr'] *= 1.01\n",
        "\n",
        "            p.data[accepted] = p_proposed.data[accepted]\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "IXwZzUAWrqPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Objects\n",
        "\n",
        "Keep track of all the optimizers and initiation through a model object as\n",
        "defined *below*"
      ],
      "metadata": {
        "id": "Zn9K5P9DrqUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Central Container\n",
        "\n",
        "contains the local models and the central model/optimizer"
      ],
      "metadata": {
        "id": "ahkP0PKS2gSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QlsdPP():\n",
        "  def __init__(self, model, partition_sizes, lr, s, l, preconditioner, prior,\n",
        "               device = 'cpu', MALA = True):\n",
        "\n",
        "    '''\n",
        "    Arguments:\n",
        "    model: Pytorch model object to be distributed\n",
        "    partition_sizes: list (int) with the sizes of each dataset\n",
        "    lr: (float) learning rate\n",
        "    s: (int > 0) number of quantization states\n",
        "    l: (int > 0) period of full gradient calculation\n",
        "    preconditioner: Preconditioner object (Euclidean RMSProp etc.)\n",
        "    prior: Prior object, prior distribution for weights\n",
        "\n",
        "    this class contains all models used, they can be simultaneously updated\n",
        "    using the .step() command after calculating the loss with a .loss(sample)\n",
        "    command.  Also zeros the gradient for each optimizer and zeros the individual\n",
        "    sample gradients.\n",
        "    '''\n",
        "\n",
        "    self.b = len(partition_sizes)\n",
        "    self.s = s\n",
        "    self.l = l\n",
        "\n",
        "    self.central_model = {'model': model.to(device),\n",
        "                          'optimizer': QlsdPPCentral(model.parameters(),\n",
        "                                                     lr, self.b, s,\n",
        "                                                     preconditioner,\n",
        "                                                     MALA = MALA)}\n",
        "    self.clock = 0\n",
        "\n",
        "\n",
        "    submodels = []\n",
        "    for partition in partition_sizes:\n",
        "\n",
        "      submodel = copy.deepcopy(model)\n",
        "      submodel = submodel.to(device)\n",
        "\n",
        "      opt = QlsdPPLocal(submodel.parameters(), prior = prior, N = partition, \n",
        "                        l = l, s = s)\n",
        "      \n",
        "      submodels.append({'model': GradSampleModule(submodel),\n",
        "                        'optimizer': opt, \n",
        "                        'N': partition,\n",
        "                        'indices': torch.arange(partition)})\n",
        "\n",
        "\n",
        "    self.distributed_model = submodels\n",
        "    self.loss_criterion = torch.nn.CrossEntropyLoss()\n",
        "    self.saved_weights = []\n",
        "    self.mix_in = 1000\n",
        "    self.active = [i for i in range(self.b)]\n",
        "\n",
        "  def step(self):\n",
        "    grads = []\n",
        "    for ind, local in enumerate(self.distributed_model):\n",
        "      if ind in self.active:\n",
        "        local['optimizer'].step(self.clock, local['indices'])\n",
        "        local_grad = []\n",
        "        for name, param in local['model'].named_parameters():\n",
        "          local_grad.append(param.grad)\n",
        "        grads.append(local_grad)\n",
        "    \n",
        "    self.central_model['optimizer'].step(grads)\n",
        "\n",
        "    for local in self.distributed_model:\n",
        "      for param1, param2 in zip(local['model'].parameters(),\n",
        "                                self.central_model['model'].parameters()):\n",
        "        param1.data = copy.deepcopy(param2.data)\n",
        "\n",
        "    self.clock += 1\n",
        "    return grads\n",
        "\n",
        "  def loss(self, sampled_data, p = 1):\n",
        "\n",
        "    loss = 0\n",
        "    active = []\n",
        "    for i,local in enumerate(self.distributed_model):\n",
        "\n",
        "      local['indices'] = None\n",
        "\n",
        "      # Require all models to take the full gradient at the same time step.\n",
        "      # Drop out during full gradient calculation steps causes problems.\n",
        "      if (torch.rand(1)<= p) or (self.clock % self.l == 0):\n",
        "\n",
        "        y_hat = local['model'].forward(sampled_data[0][i].to(device))\n",
        "        local['indices'] = sampled_data[2][i]\n",
        "        loss += self.loss_criterion(y_hat, sampled_data[1][i].to(device))\n",
        "        active.append(i)\n",
        "\n",
        "    self.active = active\n",
        "    return loss\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for local in self.distributed_model:\n",
        "      local['optimizer'].zero_grad()\n",
        "      local['model'].zero_grad()\n",
        "\n",
        "  def sample_weights(self):\n",
        "    if (self.clock >= self.mix_in) and (self.clock % 100 == 0):\n",
        "      with torch.no_grad():\n",
        "        self.saved_weights.append({name: (param.cpu().detach().clone()) for name, param in \\\n",
        "        self.central_model['model'].named_parameters()})"
      ],
      "metadata": {
        "id": "WzGUB4KD6QHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN Model, Training Loop, Finding the LR, Testing Loop"
      ],
      "metadata": {
        "id": "qhj63gML79n1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Neural Network Model"
      ],
      "metadata": {
        "id": "2Ch-xL-k227u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 10\n",
        "class SubNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SubNet, self).__init__()\n",
        "    self.layer1 = nn.Linear(input_dimension, hidden_size)\n",
        "    self.layer2 = nn.Linear(hidden_size, output_dimension)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.layer1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.layer2(x)\n",
        "    return(x)"
      ],
      "metadata": {
        "id": "23cUEEQ_6QLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding Learning Rate"
      ],
      "metadata": {
        "id": "CxEZjyVs3h97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_lr(fed_mod, data, batch_size, l, p = 1, device = 'cpu',\n",
        "            initial_value = 1e-10, final_value = 10.0):\n",
        "  '''\n",
        "  Cycles through learning rates as the dataset is sampled.  Use one with the\n",
        "  greatest decrease on the loss funciton.  If the loss blows up, break the\n",
        "  loop.  It can be useful to check filtered results by hand to select an optimal\n",
        "  LR.\n",
        "  '''\n",
        "  n_passes = int(sum(data.partition_sizes)/batch_size)\n",
        "  print('Total Number of Passes: ', n_passes)\n",
        "  step_size = (final_value/initial_value)**(1.0/n_passes)\n",
        "  training_loss = []\n",
        "  log_lrs = []\n",
        "  LR = initial_value\n",
        "  fed_mod.central_model['optimizer'].param_groups[0]['lr'] = LR\n",
        "  best_loss = 0.0\n",
        "  for t in range(n_passes):\n",
        "    fed_mod.zero_grad()\n",
        "\n",
        "    if t % l == 0:\n",
        "      sample = data.sample()\n",
        "    else:\n",
        "      sample = data.sample(batch_size)\n",
        "\n",
        "    loss = fed_mod.loss(sample, p)\n",
        "\n",
        "    if type(loss) == int:\n",
        "      continue\n",
        "    loss.backward()\n",
        "    fed_mod.step()\n",
        "\n",
        "    training_loss.append(loss.item())\n",
        "    log_lrs.append(math.log10(LR))\n",
        "\n",
        "    if t > 1 and loss > 4*best_loss:\n",
        "      print(\"Loss Exploded\")\n",
        "      break\n",
        "\n",
        "    if loss < best_loss or t == 1:\n",
        "      best_loss = loss\n",
        "\n",
        "    LR*= step_size\n",
        "    fed_mod.central_model['optimizer'].param_groups[0]['lr'] = LR\n",
        "    if t % 100 == 0:\n",
        "      print(\"Iteration %d \" % (t + 1))\n",
        "  return training_loss, log_lrs"
      ],
      "metadata": {
        "id": "MfV1wNUH6QO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "WNZDWRek3lA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(fed_mod, data, batch_size, l, max_iter, p = 1):\n",
        "  training_loss = []\n",
        "  b = len(data.partition_sizes)\n",
        "  for t in range(max_iter):\n",
        "    fed_mod.zero_grad()\n",
        "\n",
        "\n",
        "    if t % l == 0:\n",
        "      sample = data.sample()\n",
        "    else:\n",
        "      sample = data.sample(batch_size)\n",
        "\n",
        "\n",
        "    loss = fed_mod.loss(sample, p)\n",
        "\n",
        "    # int loss means no particpating nodes\n",
        "    if type(loss) == int:\n",
        "      continue\n",
        "\n",
        "    loss.backward()\n",
        "    fed_mod.step()\n",
        "\n",
        "    training_loss.append(loss.item())\n",
        "\n",
        "    if t % 100 == 0:\n",
        "      print(\"Epoch %d Loss: %f\" % (t + 1,training_loss[-1]))\n",
        "\n",
        "    if t % int(10000/b) == 0:\n",
        "      pass\n",
        "      #fed_mod.central_model['optimizer'].param_groups[0]['lr'] *= .75\n",
        "\n",
        "    if t >= 2500:\n",
        "      fed_mod.sample_weights()\n",
        "\n",
        "  return training_loss"
      ],
      "metadata": {
        "id": "n25e166K6QSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Loop"
      ],
      "metadata": {
        "id": "j_g1YCGX3sFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Test(fed_mod, data, device = 'cpu'):\n",
        "  sample = data.sample()\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  total = 0\n",
        "  num_correct = 0\n",
        "  b = len(sample[0])\n",
        "  raw_output = []\n",
        "  raw_targets = []\n",
        "  for i in range(b):\n",
        "    sample_output = fed_mod.central_model['model'](sample[0][i].to(device))\n",
        "    targets = sample[1][i].to(device)\n",
        "    losses.append(fed_mod.loss_criterion(sample_output, targets).cpu().item())\n",
        "    correct_in_sample = (1.0*(sample_output.argmax(dim = 1) == targets)).sum()\n",
        "    num_correct += correct_in_sample.cpu()\n",
        "    total += len(targets)\n",
        "    accuracies.append(correct_in_sample/len(targets))\n",
        "\n",
        "    raw_output.append(sample_output)\n",
        "    raw_targets.append(targets)\n",
        "  overall_accuracy = num_correct/total\n",
        "\n",
        "  return(losses, accuracies, overall_accuracy, torch.vstack(raw_output),\n",
        "         torch.cat(raw_targets, dim = 0))"
      ],
      "metadata": {
        "id": "Xl4Mt1ec8u7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment: Learning on MNIST with partial participation\n",
        "\n",
        "I chose MNIST since it fits on google colab and I can compare directly to results\n",
        "\n",
        "found using natural langevin dynamics."
      ],
      "metadata": {
        "id": "KonsAQonuKSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finding an optimal LR and training"
      ],
      "metadata": {
        "id": "SW34ZrSw0kpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"Cuda Found\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "8w8oxBZ2241u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e86c0e41-429f-4c0c-eba2-d433f1afce16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda Found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = CycleSplitData(MNIST, 10, transform = lambda x: x/255.)\n",
        "local_model = SubNet()\n",
        "p = 16\n",
        "l = 20\n",
        "\n",
        "fed_mod = QlsdPP(local_model,\n",
        "                 data.partition_sizes, \n",
        "                 lr = 5e-7,\n",
        "                 s = 2**p,\n",
        "                 l = l,\n",
        "                 preconditioner = Euclidean, \n",
        "                 prior = Gaussian(0,0.1), device = device,\n",
        "                 MALA = False)"
      ],
      "metadata": {
        "id": "C519sLMciWJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses, log_lrs = find_lr(fed_mod, data, batch_size, l, p = 1)"
      ],
      "metadata": {
        "id": "wXWh-HcCq9qG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5cf27cd-3d64-4591-f69a-40a97dac664a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of Passes:  600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1 \n",
            "Iteration 101 \n",
            "Iteration 201 \n",
            "Iteration 301 \n",
            "Loss Exploded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import savgol_filter\n",
        "\n",
        "training_loss_smooth = savgol_filter(np.array(losses), 101, 3)\n",
        "plt.plot(log_lrs,training_loss_smooth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "kmfUikH4uiaq",
        "outputId": "6423f010-237b-4f7a-808c-2471689c55c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4790bc9610>]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate can be finicky, close to leftmost minima\n",
        "LR_10 = log_lrs[np.argmin(np.diff(training_loss_smooth))+1]\n",
        "#LR_10 = -5 #-4.921666666666658\n",
        "#LR_10 = -2\n",
        "LR = 10**LR_10\n",
        "print('Learning Rate from Minima: ', LR)\n",
        "print('log_10(LR): ', LR_10)\n",
        "#LR = 10**(np.log10(-5.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amk8dWSiur8v",
        "outputId": "69ee7de1-610c-44f2-dcbf-538f42313275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate from Minima:  2.3085183622427994e-06\n",
            "log_10(LR):  -5.63666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_model = SubNet()\n",
        "fed_mod = QlsdPP(local_model, \n",
        "                 data.partition_sizes, \n",
        "                 lr = LR,\n",
        "                 s = 2**p,\n",
        "                 l = l,\n",
        "                 preconditioner = Adam, \n",
        "                 prior = Gaussian(0,0.1), device = device,\n",
        "                 MALA = True)"
      ],
      "metadata": {
        "id": "LmfALS3ByEDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = train(fed_mod, data, batch_size, l, 4000, p = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CV52SQxvEW3",
        "outputId": "a41793c4-8f9c-4436-fa70-cf430435ae8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 23.145533\n",
            "Epoch 101 Loss: 14.054606\n",
            "Epoch 201 Loss: 8.051249\n",
            "Epoch 301 Loss: 5.907751\n",
            "Epoch 401 Loss: 5.197047\n",
            "Epoch 501 Loss: 4.733196\n",
            "Epoch 601 Loss: 4.530804\n",
            "Epoch 701 Loss: 4.363981\n",
            "Epoch 801 Loss: 4.263237\n",
            "Epoch 901 Loss: 4.174806\n",
            "Epoch 1001 Loss: 4.126699\n",
            "Epoch 1101 Loss: 4.041432\n",
            "Epoch 1201 Loss: 4.026085\n",
            "Epoch 1301 Loss: 4.008116\n",
            "Epoch 1401 Loss: 3.971715\n",
            "Epoch 1501 Loss: 3.938886\n",
            "Epoch 1601 Loss: 3.948369\n",
            "Epoch 1701 Loss: 3.909026\n",
            "Epoch 1801 Loss: 3.933086\n",
            "Epoch 1901 Loss: 3.897209\n",
            "Epoch 2001 Loss: 3.860873\n",
            "Epoch 2101 Loss: 3.889560\n",
            "Epoch 2201 Loss: 3.893169\n",
            "Epoch 2301 Loss: 3.887274\n",
            "Epoch 2401 Loss: 3.860960\n",
            "Epoch 2501 Loss: 3.817187\n",
            "Epoch 2601 Loss: 3.847632\n",
            "Epoch 2701 Loss: 3.833492\n",
            "Epoch 2801 Loss: 3.870532\n",
            "Epoch 2901 Loss: 3.848746\n",
            "Epoch 3001 Loss: 3.834824\n",
            "Epoch 3101 Loss: 3.811452\n",
            "Epoch 3201 Loss: 3.814698\n",
            "Epoch 3301 Loss: 3.816237\n",
            "Epoch 3401 Loss: 3.838049\n",
            "Epoch 3501 Loss: 3.813815\n",
            "Epoch 3601 Loss: 3.842110\n",
            "Epoch 3701 Loss: 3.833281\n",
            "Epoch 3801 Loss: 3.841797\n",
            "Epoch 3901 Loss: 3.795777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(training_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "NgtDbpm_-9IV",
        "outputId": "4c38914a-e386-4b36-a9f0-9ba1d5498884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4790a5cdd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, train_accuracies, train_overall_accuracy, \\\n",
        "raw_out, raw_labels = Test(fed_mod, data, device)"
      ],
      "metadata": {
        "id": "8RyqWlL3vNPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST_test = torchvision.datasets.MNIST(root = \".\",\n",
        "                                        train = False,\n",
        "                                        download = True)\n",
        "\n",
        "data_test = CycleSplitData(MNIST_test,10, transform = lambda x: x/255.)\n",
        "\n",
        "FMNIST_test = torchvision.datasets.FashionMNIST(root = '.',\n",
        "                                                train = False,\n",
        "                                                download = True)\n",
        "\n",
        "data_test_oos = CycleSplitData(FMNIST_test, 10, transform = lambda x: x/255.)"
      ],
      "metadata": {
        "id": "uvyNgdjOyfXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses, test_accuracies, test_overall_accuracy\\\n",
        ",raw_test_out, raw_test_labels = Test(fed_mod, data_test, device)"
      ],
      "metadata": {
        "id": "8UZJ4dtHyg4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvDg_qBa-lYM",
        "outputId": "33f8c424-9060-4903-91ba-85847131ce3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(0.9037, device='cuda:0'), tensor(0.8725, device='cuda:0'), tensor(0.8881, device='cuda:0'), tensor(0.8896, device='cuda:0'), tensor(0.8822, device='cuda:0'), tensor(0.9218, device='cuda:0'), tensor(0.9264, device='cuda:0'), tensor(0.9448, device='cuda:0'), tensor(0.9634, device='cuda:0'), tensor(0.9147, device='cuda:0')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Overall test accuracy is comparable to similar model in non-federated context\n",
        "print(test_overall_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFDQRkyF-ogf",
        "outputId": "35f7e1ba-bfa3-404f-f748-524ca10ce8c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(test_losses)/len(test_losses))"
      ],
      "metadata": {
        "id": "EEnmNPQnJCKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda84c21-48f8-43c0-d22c-f03dad3f33aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3685747891664505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting model averaged weights"
      ],
      "metadata": {
        "id": "USiK7-pL0bsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_mean(saved_weights):\n",
        "  saved_weights = fed_mod.saved_weights\n",
        "  post_mean = dict.fromkeys(saved_weights[0], 0)\n",
        "  for weight in saved_weights:\n",
        "    for layername in weight:\n",
        "      post_mean[layername] += weight[layername]/len(saved_weights)\n",
        "  return post_mean"
      ],
      "metadata": {
        "id": "HPZodn4Z0Srx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fed_mod.saved_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSb-hMT_j6as",
        "outputId": "31642092-dbd0-4dc5-abb7-0b25459ee83c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'layer1.bias': tensor([-0.0820, -0.0570,  0.1179,  0.0444,  0.2138, -0.1720,  0.0856,  0.2911,\n",
              "           0.2149,  0.0380]),\n",
              "  'layer1.weight': tensor([[ 0.0555,  0.0534, -0.0414,  ...,  0.0126,  0.0290,  0.0136],\n",
              "          [ 0.0172, -0.0057,  0.0037,  ...,  0.0145, -0.0006,  0.0118],\n",
              "          [ 0.0093, -0.0184,  0.0078,  ..., -0.0155, -0.0177, -0.0210],\n",
              "          ...,\n",
              "          [ 0.0378,  0.0229, -0.0140,  ..., -0.0345,  0.0228, -0.0035],\n",
              "          [ 0.0224,  0.0081, -0.0128,  ..., -0.0413,  0.0316, -0.0062],\n",
              "          [ 0.0450, -0.0067, -0.0194,  ..., -0.0205, -0.0014,  0.0259]]),\n",
              "  'layer2.bias': tensor([-0.0913,  0.0953, -0.0465, -0.0119,  0.0543,  0.1901, -0.0078,  0.0172,\n",
              "          -0.1960,  0.0874]),\n",
              "  'layer2.weight': tensor([[ 0.6122,  0.0827, -0.2945, -0.4838,  0.5611, -0.0881, -0.5872,  0.1545,\n",
              "           -0.2957, -0.1830],\n",
              "          [-0.6090,  0.0840,  0.7692,  0.6660, -0.1005, -0.2792, -0.3290, -0.2714,\n",
              "            0.2713, -0.1585],\n",
              "          [ 0.4402,  0.1224, -0.1037,  0.3488, -0.1222, -0.4814, -0.2066, -0.0663,\n",
              "           -0.3548,  0.8790],\n",
              "          [-0.2382,  0.3889,  0.3501, -0.5481, -0.0387,  0.3739, -0.3345, -0.3033,\n",
              "            0.3799,  0.7683],\n",
              "          [-0.0453, -0.4710,  0.2986, -0.4941, -0.3333, -0.3001,  1.0592,  0.1346,\n",
              "           -0.1418, -0.2341],\n",
              "          [-0.2897, -0.3780, -0.3694, -0.0284,  0.9749,  0.1826,  0.2699, -0.3092,\n",
              "            0.2120,  0.1146],\n",
              "          [ 0.7160, -0.4484, -0.2623,  0.2523, -0.2346, -0.5527,  0.0656, -0.1710,\n",
              "            0.6061, -0.4171],\n",
              "          [-0.5470, -0.1166,  0.0233,  0.0489, -0.1935,  0.1872, -0.4346,  1.1346,\n",
              "           -0.1891,  0.1925],\n",
              "          [ 0.2242, -0.0035, -0.1050,  0.3698, -0.0785,  0.7740,  0.1494, -0.5486,\n",
              "           -0.5736, -0.3018],\n",
              "          [-0.2833,  0.8560, -0.4847, -0.1814, -0.4738,  0.1948,  0.3042,  0.2985,\n",
              "           -0.0516, -0.4925]])},\n",
              " {'layer1.bias': tensor([-0.1208,  0.0021,  0.1228,  0.0617,  0.1794, -0.1251,  0.0549,  0.2657,\n",
              "           0.1469, -0.0093]),\n",
              "  'layer1.weight': tensor([[ 0.0423,  0.0564, -0.0544,  ...,  0.0183,  0.0630,  0.0176],\n",
              "          [ 0.0223, -0.0086,  0.0008,  ...,  0.0074, -0.0014,  0.0096],\n",
              "          [ 0.0034, -0.0132,  0.0027,  ..., -0.0217, -0.0146, -0.0190],\n",
              "          ...,\n",
              "          [ 0.0231,  0.0299, -0.0002,  ..., -0.0412,  0.0271, -0.0049],\n",
              "          [ 0.0294,  0.0112, -0.0069,  ..., -0.0499,  0.0230, -0.0201],\n",
              "          [ 0.0406,  0.0014,  0.0018,  ..., -0.0444,  0.0254,  0.0390]]),\n",
              "  'layer2.bias': tensor([-0.0894,  0.1386, -0.0219, -0.0306,  0.0625,  0.1650,  0.0235,  0.0405,\n",
              "          -0.2123,  0.0637]),\n",
              "  'layer2.weight': tensor([[ 0.7200, -0.0999, -0.3020, -0.5302,  0.5299, -0.0629, -0.6271,  0.2166,\n",
              "           -0.2833, -0.1301],\n",
              "          [-0.6005,  0.2359,  0.7321,  0.6870, -0.0349, -0.1821, -0.4412, -0.3209,\n",
              "            0.2597, -0.1914],\n",
              "          [ 0.3932,  0.1135, -0.0279,  0.3857, -0.1626, -0.5055, -0.1843, -0.0377,\n",
              "           -0.3051,  0.8892],\n",
              "          [-0.2283,  0.3854,  0.2645, -0.4649, -0.0256,  0.4319, -0.3038, -0.3731,\n",
              "            0.3518,  0.8049],\n",
              "          [-0.1554, -0.3770,  0.3647, -0.4441, -0.3048, -0.2816,  1.0016,  0.2615,\n",
              "           -0.1823, -0.2616],\n",
              "          [-0.2669, -0.3688, -0.3312, -0.1158,  1.0383,  0.0983,  0.3173, -0.2525,\n",
              "            0.1809,  0.0658],\n",
              "          [ 0.6030, -0.4067, -0.0230,  0.2510, -0.2990, -0.6031,  0.1512, -0.1212,\n",
              "            0.7193, -0.4512],\n",
              "          [-0.5730, -0.1167,  0.0353,  0.0538, -0.3209,  0.2416, -0.4860,  1.1013,\n",
              "           -0.1308,  0.2617],\n",
              "          [ 0.2490, -0.1214, -0.1466,  0.4023, -0.0523,  0.6945,  0.1832, -0.5561,\n",
              "           -0.5719, -0.2589],\n",
              "          [-0.2334,  0.8449, -0.5238, -0.2492, -0.4759,  0.1827,  0.2796,  0.3033,\n",
              "           -0.0207, -0.5161]])},\n",
              " {'layer1.bias': tensor([-0.0549, -0.0814,  0.0793,  0.0936,  0.1632, -0.1360,  0.1003,  0.2089,\n",
              "           0.1743,  0.0286]),\n",
              "  'layer1.weight': tensor([[ 0.0412,  0.0317, -0.0716,  ..., -0.0052,  0.0673,  0.0397],\n",
              "          [ 0.0227, -0.0061,  0.0039,  ...,  0.0066,  0.0005,  0.0072],\n",
              "          [ 0.0088, -0.0072,  0.0141,  ..., -0.0118, -0.0048, -0.0261],\n",
              "          ...,\n",
              "          [-0.0045,  0.0406, -0.0011,  ..., -0.0247,  0.0429, -0.0353],\n",
              "          [ 0.0328,  0.0088, -0.0005,  ..., -0.0358,  0.0172, -0.0160],\n",
              "          [ 0.0085,  0.0173, -0.0092,  ..., -0.0312,  0.0188,  0.0165]]),\n",
              "  'layer2.bias': tensor([-0.0549,  0.1305, -0.0549, -0.0205,  0.0736,  0.1429,  0.0019,  0.0961,\n",
              "          -0.2573,  0.0294]),\n",
              "  'layer2.weight': tensor([[ 0.6207,  0.0224, -0.2971, -0.5136,  0.7108, -0.0926, -0.6075,  0.1549,\n",
              "           -0.3797, -0.1129],\n",
              "          [-0.5792, -0.0262,  0.7870,  0.7098, -0.1915, -0.2271, -0.3619, -0.3132,\n",
              "            0.2710, -0.1624],\n",
              "          [ 0.4636,  0.1214, -0.0904,  0.3660, -0.1473, -0.4959, -0.1949, -0.0946,\n",
              "           -0.3861,  0.9174],\n",
              "          [-0.2245,  0.3258,  0.4258, -0.5293, -0.0773,  0.3973, -0.3262, -0.2553,\n",
              "            0.3580,  0.7435],\n",
              "          [-0.0521, -0.4858,  0.1931, -0.4313, -0.2946, -0.3200,  1.0148,  0.1887,\n",
              "           -0.1475, -0.1773],\n",
              "          [-0.2975, -0.3593, -0.3646, -0.0457,  0.9451,  0.1439,  0.2686, -0.3262,\n",
              "            0.2324,  0.0607],\n",
              "          [ 0.7206, -0.4462, -0.2182,  0.2470, -0.2568, -0.5525,  0.0663, -0.1726,\n",
              "            0.4763, -0.4183],\n",
              "          [-0.5590, -0.1124, -0.0039,  0.0577, -0.2706,  0.1869, -0.3970,  1.1310,\n",
              "           -0.1498,  0.2174],\n",
              "          [ 0.2062,  0.0443, -0.1261,  0.3538, -0.0382,  0.8013,  0.1250, -0.5587,\n",
              "           -0.5273, -0.2508],\n",
              "          [-0.2587,  0.8589, -0.4262, -0.2322, -0.4479,  0.1232,  0.3006,  0.2786,\n",
              "            0.1032, -0.4929]])},\n",
              " {'layer1.bias': tensor([-0.0601,  0.0079,  0.0793,  0.0706,  0.1674, -0.0938,  0.0688,  0.2065,\n",
              "           0.1664,  0.0257]),\n",
              "  'layer1.weight': tensor([[ 0.0259,  0.0302, -0.0993,  ..., -0.0253,  0.1141,  0.0321],\n",
              "          [ 0.0187, -0.0084,  0.0008,  ..., -0.0066,  0.0147,  0.0043],\n",
              "          [ 0.0118, -0.0091,  0.0193,  ..., -0.0066,  0.0022, -0.0185],\n",
              "          ...,\n",
              "          [-0.0086,  0.0443,  0.0001,  ..., -0.0413,  0.0492, -0.0294],\n",
              "          [-0.0025,  0.0012,  0.0435,  ..., -0.0268,  0.0221,  0.0077],\n",
              "          [ 0.0131,  0.0276, -0.0338,  ..., -0.0337,  0.0149,  0.0299]]),\n",
              "  'layer2.bias': tensor([-0.0945,  0.1267, -0.0772, -0.0098, -0.0112,  0.2090,  0.0129,  0.0831,\n",
              "          -0.2849, -0.0209]),\n",
              "  'layer2.weight': tensor([[ 0.6879, -0.1866, -0.3064, -0.4946,  0.5856, -0.0824, -0.6048,  0.1500,\n",
              "           -0.2737, -0.0696],\n",
              "          [-0.5995,  0.2629,  0.7052,  0.7265, -0.1249, -0.2158, -0.4497, -0.3421,\n",
              "            0.2360, -0.1957],\n",
              "          [ 0.4401,  0.1538, -0.0210,  0.3524, -0.1343, -0.7153, -0.1734, -0.0812,\n",
              "           -0.3244,  0.8966],\n",
              "          [-0.1880,  0.3069,  0.3144, -0.4966, -0.0442,  0.4295, -0.2965, -0.4095,\n",
              "            0.3491,  0.7693],\n",
              "          [-0.1058, -0.4845,  0.3883, -0.4349, -0.3182, -0.2740,  0.9811,  0.2397,\n",
              "           -0.1356, -0.3042],\n",
              "          [-0.2448, -0.3953, -0.3601, -0.0897,  0.9043,  0.1274,  0.2785, -0.2729,\n",
              "            0.2178,  0.0105],\n",
              "          [ 0.6394, -0.3296, -0.1640,  0.2533, -0.2926, -0.6212,  0.1539, -0.1748,\n",
              "            0.6102, -0.4351],\n",
              "          [-0.5610, -0.1636, -0.0106,  0.0170, -0.3237,  0.2625, -0.5239,  1.0772,\n",
              "           -0.0656,  0.3090],\n",
              "          [ 0.2677, -0.0972, -0.1916,  0.3980, -0.0441,  0.6796,  0.1697, -0.5792,\n",
              "           -0.5860, -0.2538],\n",
              "          [-0.1989,  0.8303, -0.4175, -0.3322, -0.4213,  0.2039,  0.2337,  0.2974,\n",
              "           -0.0952, -0.4818]])},\n",
              " {'layer1.bias': tensor([-0.1074, -0.0087,  0.0724,  0.1052,  0.1509, -0.1531,  0.0780,  0.1950,\n",
              "           0.1783,  0.0194]),\n",
              "  'layer1.weight': tensor([[ 0.0273,  0.0294, -0.0784,  ..., -0.0089,  0.0953,  0.0068],\n",
              "          [ 0.0178, -0.0104,  0.0009,  ..., -0.0001,  0.0133, -0.0038],\n",
              "          [ 0.0113,  0.0069,  0.0206,  ..., -0.0125,  0.0140, -0.0100],\n",
              "          ...,\n",
              "          [-0.0021,  0.0265, -0.0011,  ..., -0.0581,  0.0339, -0.0390],\n",
              "          [ 0.0059,  0.0189,  0.0602,  ..., -0.0233,  0.0002,  0.0257],\n",
              "          [ 0.0292,  0.0214, -0.0189,  ..., -0.0205,  0.0134,  0.0607]]),\n",
              "  'layer2.bias': tensor([-0.0456,  0.0859, -0.0584,  0.0050, -0.0184,  0.1819,  0.0177,  0.0406,\n",
              "          -0.3112, -0.0326]),\n",
              "  'layer2.weight': tensor([[ 0.6602,  0.0825, -0.3066, -0.5191,  0.6432, -0.0986, -0.6268,  0.1000,\n",
              "           -0.3440, -0.1339],\n",
              "          [-0.5715,  0.0506,  0.8109,  0.7341, -0.1712, -0.3738, -0.3800, -0.2926,\n",
              "            0.2750, -0.1875],\n",
              "          [ 0.4321,  0.1059, -0.0857,  0.3639, -0.1193, -0.5523, -0.2069, -0.0913,\n",
              "           -0.3645,  0.9250],\n",
              "          [-0.2211,  0.2810,  0.4613, -0.5263, -0.0453,  0.3966, -0.3147, -0.3491,\n",
              "            0.3486,  0.7140],\n",
              "          [ 0.0463, -0.4955,  0.1897, -0.4561, -0.3627, -0.3103,  0.9764,  0.2217,\n",
              "           -0.1518, -0.2775],\n",
              "          [-0.2977, -0.3946, -0.4154, -0.0218,  0.9129,  0.1927,  0.2555, -0.3145,\n",
              "            0.2150,  0.0161],\n",
              "          [ 0.6884, -0.3999, -0.1554,  0.2732, -0.2813, -0.5671,  0.0137, -0.1960,\n",
              "            0.4657, -0.4002],\n",
              "          [-0.5301, -0.1473, -0.0201,  0.0282, -0.2594,  0.1604, -0.4755,  1.1246,\n",
              "           -0.1120,  0.2702],\n",
              "          [ 0.2352,  0.0088, -0.1163,  0.3727, -0.0442,  0.7568,  0.1426, -0.5568,\n",
              "           -0.5769, -0.2611],\n",
              "          [-0.2187,  0.8695, -0.3391, -0.2464, -0.5037,  0.1510,  0.2603,  0.2469,\n",
              "            0.0016, -0.4630]])},\n",
              " {'layer1.bias': tensor([-0.1113,  0.0157,  0.1080,  0.1014,  0.1294, -0.0786,  0.0697,  0.2570,\n",
              "           0.1512, -0.0244]),\n",
              "  'layer1.weight': tensor([[ 0.0285,  0.0026, -0.0882,  ..., -0.0099,  0.1057,  0.0300],\n",
              "          [ 0.0177, -0.0030,  0.0002,  ...,  0.0038,  0.0181, -0.0066],\n",
              "          [ 0.0066,  0.0108,  0.0272,  ..., -0.0102,  0.0189, -0.0014],\n",
              "          ...,\n",
              "          [-0.0109,  0.0288,  0.0128,  ..., -0.0521,  0.0504, -0.0428],\n",
              "          [ 0.0049, -0.0041,  0.0470,  ..., -0.0073,  0.0388,  0.0219],\n",
              "          [ 0.0122,  0.0282, -0.0469,  ..., -0.0047, -0.0043,  0.0274]]),\n",
              "  'layer2.bias': tensor([-0.0624,  0.1361, -0.0317, -0.0240,  0.0113,  0.1910,  0.0132,  0.0730,\n",
              "          -0.2813, -0.0659]),\n",
              "  'layer2.weight': tensor([[ 0.7201, -0.1506, -0.3017, -0.5674,  0.5737, -0.0964, -0.6396,  0.1201,\n",
              "           -0.2417, -0.0755],\n",
              "          [-0.6085,  0.2067,  0.6978,  0.7643, -0.1818, -0.2645, -0.3882, -0.3455,\n",
              "            0.2295, -0.2141],\n",
              "          [ 0.4359,  0.1192,  0.0065,  0.3740, -0.1453, -0.6155, -0.1691, -0.0570,\n",
              "           -0.2853,  0.9091],\n",
              "          [-0.2010,  0.2743,  0.3060, -0.5026, -0.0240,  0.4256, -0.2917, -0.4318,\n",
              "            0.3332,  0.8211],\n",
              "          [-0.0934, -0.3921,  0.3525, -0.4246, -0.3542, -0.2926,  0.9243,  0.2517,\n",
              "           -0.1503, -0.2707],\n",
              "          [-0.3298, -0.3776, -0.3190, -0.0548,  0.9756,  0.0958,  0.2879, -0.2518,\n",
              "            0.2904,  0.0142],\n",
              "          [ 0.5785, -0.3531, -0.0466,  0.2678, -0.2786, -0.6527,  0.1225, -0.1659,\n",
              "            0.7201, -0.4438],\n",
              "          [-0.5666, -0.1331, -0.0298,  0.0160, -0.2840,  0.2573, -0.5245,  1.1130,\n",
              "           -0.0946,  0.2577],\n",
              "          [ 0.2901, -0.0868, -0.2286,  0.3713, -0.0259,  0.6266,  0.1619, -0.5690,\n",
              "           -0.6353, -0.2699],\n",
              "          [-0.1560,  0.8313, -0.3794, -0.3558, -0.4426,  0.2365,  0.2158,  0.2791,\n",
              "           -0.1962, -0.4748]])},\n",
              " {'layer1.bias': tensor([-0.0702,  0.0041,  0.0965,  0.1117,  0.1570, -0.1148,  0.1121,  0.2738,\n",
              "           0.2099,  0.0323]),\n",
              "  'layer1.weight': tensor([[ 0.0366, -0.0116, -0.0950,  ...,  0.0204,  0.0737,  0.0234],\n",
              "          [ 0.0271,  0.0008, -0.0011,  ...,  0.0039,  0.0048, -0.0084],\n",
              "          [-0.0027,  0.0101,  0.0406,  ..., -0.0070,  0.0156, -0.0079],\n",
              "          ...,\n",
              "          [ 0.0360,  0.0257, -0.0111,  ..., -0.0396,  0.0280, -0.0460],\n",
              "          [ 0.0023, -0.0072,  0.0478,  ...,  0.0114,  0.0248,  0.0309],\n",
              "          [ 0.0212,  0.0267, -0.0389,  ..., -0.0044,  0.0263,  0.0232]]),\n",
              "  'layer2.bias': tensor([ 0.0223,  0.1245, -0.0476, -0.0467,  0.0174,  0.1676,  0.0256,  0.0853,\n",
              "          -0.2871, -0.0258]),\n",
              "  'layer2.weight': tensor([[ 0.6221,  0.0808, -0.2758, -0.5634,  0.6162, -0.0556, -0.6148,  0.1387,\n",
              "           -0.3170, -0.0775],\n",
              "          [-0.5918, -0.0624,  0.7530,  0.7210, -0.2035, -0.3380, -0.3322, -0.2600,\n",
              "            0.3059, -0.1853],\n",
              "          [ 0.4433,  0.0817, -0.1403,  0.3662, -0.1268, -0.4843, -0.1908, -0.0910,\n",
              "           -0.3125,  0.9474],\n",
              "          [-0.2140,  0.2490,  0.4813, -0.5554, -0.0518,  0.3989, -0.3109, -0.3620,\n",
              "            0.3378,  0.7509],\n",
              "          [ 0.0068, -0.4442,  0.1892, -0.4224, -0.3824, -0.3511,  0.9731,  0.2209,\n",
              "           -0.1807, -0.2285],\n",
              "          [-0.3764, -0.3720, -0.3980,  0.0088,  0.9710,  0.1553,  0.2818, -0.2832,\n",
              "            0.2744,  0.0014],\n",
              "          [ 0.7960, -0.3745, -0.2125,  0.2529, -0.2764, -0.6084,  0.0323, -0.2077,\n",
              "            0.6195, -0.4537],\n",
              "          [-0.5347, -0.1575, -0.0450,  0.0196, -0.2421,  0.1489, -0.4653,  1.1937,\n",
              "           -0.1435,  0.2238],\n",
              "          [ 0.2059, -0.0128, -0.1347,  0.3508, -0.0111,  0.7555,  0.1744, -0.5474,\n",
              "           -0.5917, -0.2574],\n",
              "          [-0.2023,  0.8456, -0.3207, -0.2978, -0.4601,  0.1712,  0.2810,  0.2592,\n",
              "           -0.0151, -0.4526]])},\n",
              " {'layer1.bias': tensor([-0.1387,  0.0204,  0.1018,  0.0869,  0.2117, -0.1138,  0.1085,  0.2281,\n",
              "           0.1884,  0.0206]),\n",
              "  'layer1.weight': tensor([[-0.0008, -0.0103, -0.0849,  ...,  0.0258,  0.0386,  0.0115],\n",
              "          [ 0.0203, -0.0008, -0.0041,  ...,  0.0054,  0.0104, -0.0085],\n",
              "          [ 0.0037,  0.0140,  0.0313,  ..., -0.0155,  0.0095, -0.0117],\n",
              "          ...,\n",
              "          [ 0.0386,  0.0053, -0.0027,  ..., -0.0128, -0.0246, -0.0313],\n",
              "          [-0.0161, -0.0213,  0.0540,  ...,  0.0248,  0.0215,  0.0141],\n",
              "          [-0.0117,  0.0178, -0.0029,  ..., -0.0221,  0.0195,  0.0290]]),\n",
              "  'layer2.bias': tensor([-0.0509,  0.1207, -0.0440, -0.0411,  0.0454,  0.1806, -0.0131,  0.0319,\n",
              "          -0.2424,  0.0052]),\n",
              "  'layer2.weight': tensor([[ 0.7604, -0.1945, -0.2694, -0.6010,  0.5826, -0.0918, -0.6322,  0.1441,\n",
              "           -0.2154, -0.0521],\n",
              "          [-0.6287,  0.1343,  0.6689,  0.7374, -0.1468, -0.2029, -0.3769, -0.2895,\n",
              "            0.2496, -0.2701],\n",
              "          [ 0.3997,  0.0792, -0.0688,  0.3907, -0.1362, -0.5782, -0.1700, -0.0462,\n",
              "           -0.2560,  0.9235],\n",
              "          [-0.1954,  0.2580,  0.3441, -0.4671, -0.0107,  0.4474, -0.3213, -0.4092,\n",
              "            0.3384,  0.7671],\n",
              "          [-0.1118, -0.4061,  0.3221, -0.3980, -0.3665, -0.2800,  0.9480,  0.3078,\n",
              "           -0.1716, -0.2494],\n",
              "          [-0.3314, -0.3367, -0.3783, -0.0820,  0.9775,  0.1017,  0.2896, -0.2387,\n",
              "            0.2571,  0.0033],\n",
              "          [ 0.6453, -0.3100, -0.1619,  0.2494, -0.3156, -0.5920,  0.1591, -0.1897,\n",
              "            0.6373, -0.4856],\n",
              "          [-0.5423, -0.1310, -0.0559,  0.0235, -0.2901,  0.3008, -0.5207,  1.1173,\n",
              "           -0.1385,  0.2487],\n",
              "          [ 0.3139, -0.1444, -0.2104,  0.3680, -0.0421,  0.6913,  0.1667, -0.5287,\n",
              "           -0.6546, -0.2704],\n",
              "          [-0.1999,  0.8396, -0.4871, -0.3532, -0.4394,  0.1686,  0.2412,  0.2973,\n",
              "           -0.1324, -0.4597]])},\n",
              " {'layer1.bias': tensor([-0.1120, -0.0317,  0.0983,  0.1358,  0.2056, -0.1321,  0.1331,  0.2362,\n",
              "           0.1538,  0.0246]),\n",
              "  'layer1.weight': tensor([[ 0.0268, -0.0195, -0.0430,  ...,  0.0081,  0.0340,  0.0227],\n",
              "          [ 0.0263, -0.0035,  0.0023,  ...,  0.0022,  0.0082, -0.0036],\n",
              "          [ 0.0121,  0.0044,  0.0206,  ..., -0.0148,  0.0124, -0.0077],\n",
              "          ...,\n",
              "          [ 0.0114, -0.0013, -0.0369,  ..., -0.0117, -0.0133, -0.0297],\n",
              "          [-0.0086, -0.0088,  0.0213,  ...,  0.0295,  0.0290, -0.0086],\n",
              "          [-0.0245,  0.0085, -0.0175,  ..., -0.0233,  0.0061,  0.0115]]),\n",
              "  'layer2.bias': tensor([-0.0077,  0.1228, -0.0786, -0.0690,  0.0203,  0.1899,  0.0047,  0.0652,\n",
              "          -0.2525, -0.0050]),\n",
              "  'layer2.weight': tensor([[ 0.5740,  0.0600, -0.2682, -0.5430,  0.6064, -0.0828, -0.6317,  0.1166,\n",
              "           -0.2886, -0.1179],\n",
              "          [-0.6048,  0.0433,  0.7575,  0.7585, -0.2105, -0.3601, -0.3713, -0.2785,\n",
              "            0.3040, -0.2058],\n",
              "          [ 0.4261,  0.1146, -0.1465,  0.3637, -0.1371, -0.4172, -0.2273, -0.0941,\n",
              "           -0.2768,  0.9388],\n",
              "          [-0.2422,  0.2254,  0.4883, -0.4881, -0.0354,  0.4375, -0.3141, -0.3139,\n",
              "            0.3456,  0.7557],\n",
              "          [ 0.0311, -0.4667,  0.2106, -0.4106, -0.3933, -0.3243,  0.9930,  0.1621,\n",
              "           -0.1417, -0.2370],\n",
              "          [-0.3994, -0.3576, -0.3786, -0.0113,  0.8991,  0.0910,  0.3065, -0.2524,\n",
              "            0.2784,  0.0210],\n",
              "          [ 0.7530, -0.3985, -0.1634,  0.2373, -0.2981, -0.5425, -0.0110, -0.1892,\n",
              "            0.5575, -0.4729],\n",
              "          [-0.5464, -0.0984, -0.0504,  0.0373, -0.2508,  0.1896, -0.4343,  1.1714,\n",
              "           -0.1755,  0.2175],\n",
              "          [ 0.1858, -0.0059, -0.0635,  0.3229, -0.0696,  0.7534,  0.1798, -0.5303,\n",
              "           -0.6463, -0.2718],\n",
              "          [-0.2469,  0.8389, -0.2939, -0.3271, -0.4888,  0.1352,  0.2800,  0.3241,\n",
              "            0.0378, -0.4823]])},\n",
              " {'layer1.bias': tensor([-0.1556,  0.0296,  0.1120,  0.1574,  0.1706, -0.1152,  0.1199,  0.2718,\n",
              "           0.2246,  0.0155]),\n",
              "  'layer1.weight': tensor([[ 0.0549, -0.0169, -0.0067,  ..., -0.0017,  0.0318,  0.0354],\n",
              "          [ 0.0258, -0.0027,  0.0044,  ...,  0.0033,  0.0063, -0.0017],\n",
              "          [ 0.0156,  0.0092,  0.0265,  ..., -0.0166,  0.0125,  0.0085],\n",
              "          ...,\n",
              "          [-0.0227,  0.0130, -0.0393,  ..., -0.0337,  0.0020,  0.0069],\n",
              "          [-0.0039, -0.0133,  0.0195,  ...,  0.0117,  0.0187,  0.0017],\n",
              "          [ 0.0062, -0.0059, -0.0131,  ..., -0.0209,  0.0133,  0.0196]]),\n",
              "  'layer2.bias': tensor([-0.0447,  0.0661, -0.0857, -0.0934,  0.0947,  0.1569, -0.0303,  0.1114,\n",
              "          -0.2447, -0.0053]),\n",
              "  'layer2.weight': tensor([[ 0.7385, -0.1186, -0.2329, -0.5815,  0.5544, -0.0944, -0.6499,  0.1812,\n",
              "           -0.1645, -0.0561],\n",
              "          [-0.6404,  0.0908,  0.6756,  0.7824, -0.1694, -0.1834, -0.4091, -0.2918,\n",
              "            0.2260, -0.2425],\n",
              "          [ 0.4287,  0.1116, -0.0637,  0.4196, -0.1593, -0.5464, -0.1536, -0.0709,\n",
              "           -0.2152,  0.9330],\n",
              "          [-0.2082,  0.2411,  0.4005, -0.4283, -0.0339,  0.4640, -0.3165, -0.4009,\n",
              "            0.3192,  0.8154],\n",
              "          [-0.0355, -0.4151,  0.3581, -0.4076, -0.4028, -0.2890,  0.9608,  0.2449,\n",
              "           -0.1375, -0.2493],\n",
              "          [-0.3375, -0.3503, -0.3958, -0.0545,  0.9585,  0.0678,  0.2949, -0.2126,\n",
              "            0.3069, -0.0349],\n",
              "          [ 0.6545, -0.3440, -0.1146,  0.2254, -0.3805, -0.6129,  0.1419, -0.1990,\n",
              "            0.7360, -0.4673],\n",
              "          [-0.5640, -0.1238, -0.0217,  0.0452, -0.3511,  0.3133, -0.4877,  1.0936,\n",
              "           -0.1909,  0.2473],\n",
              "          [ 0.2780, -0.1097, -0.1540,  0.3550, -0.0410,  0.6641,  0.2219, -0.5170,\n",
              "           -0.6731, -0.2794],\n",
              "          [-0.2056,  0.8311, -0.3733, -0.3316, -0.4400,  0.1360,  0.2978,  0.3420,\n",
              "           -0.0743, -0.4521]])},\n",
              " {'layer1.bias': tensor([-0.1387, -0.0227,  0.0615,  0.1736,  0.1754, -0.1292,  0.0922,  0.2680,\n",
              "           0.2322,  0.0351]),\n",
              "  'layer1.weight': tensor([[ 0.0746, -0.0058, -0.0142,  ..., -0.0057, -0.0086,  0.0088],\n",
              "          [ 0.0329, -0.0022,  0.0045,  ..., -0.0040,  0.0081,  0.0008],\n",
              "          [ 0.0110,  0.0097,  0.0170,  ..., -0.0088,  0.0118,  0.0139],\n",
              "          ...,\n",
              "          [ 0.0433,  0.0107, -0.0475,  ..., -0.0462, -0.0080,  0.0292],\n",
              "          [-0.0064, -0.0125,  0.0052,  ..., -0.0002,  0.0033,  0.0004],\n",
              "          [ 0.0369, -0.0262, -0.0328,  ..., -0.0151, -0.0153,  0.0240]]),\n",
              "  'layer2.bias': tensor([-0.0949,  0.0970, -0.0758, -0.0752,  0.0919,  0.1626,  0.0255,  0.0670,\n",
              "          -0.2408,  0.0102]),\n",
              "  'layer2.weight': tensor([[ 6.4407e-01,  6.5775e-02, -2.7538e-01, -5.4278e-01,  6.4908e-01,\n",
              "           -8.2504e-02, -6.2664e-01,  1.5094e-01, -2.5186e-01, -1.2955e-01],\n",
              "          [-6.3361e-01, -2.4279e-03,  6.6728e-01,  7.8766e-01, -2.4873e-01,\n",
              "           -2.5554e-01, -3.2960e-01, -2.9495e-01,  2.4216e-01, -2.4630e-01],\n",
              "          [ 4.3379e-01,  9.4591e-02, -1.4478e-01,  4.0590e-01, -1.1452e-01,\n",
              "           -4.6902e-01, -1.6609e-01, -7.1819e-02, -3.0976e-01,  9.7565e-01],\n",
              "          [-2.2885e-01,  1.9795e-01,  4.5831e-01, -4.9330e-01, -2.7923e-02,\n",
              "            4.4201e-01, -3.0638e-01, -2.9188e-01,  3.6097e-01,  7.4698e-01],\n",
              "          [ 2.7644e-02, -4.2989e-01,  2.9969e-01, -4.2170e-01, -3.6957e-01,\n",
              "           -3.1871e-01,  1.0474e+00,  1.9528e-01, -1.6477e-01, -2.0849e-01],\n",
              "          [-3.7796e-01, -3.6453e-01, -3.5558e-01, -3.7991e-02,  8.9561e-01,\n",
              "            1.1640e-01,  2.7082e-01, -2.1054e-01,  3.1836e-01, -5.8204e-04],\n",
              "          [ 7.1501e-01, -3.8534e-01, -1.4699e-01,  2.0065e-01, -3.1208e-01,\n",
              "           -5.2627e-01,  1.1636e-01, -1.9747e-01,  6.7029e-01, -4.4669e-01],\n",
              "          [-5.6062e-01, -8.7929e-02,  6.2936e-04,  5.9192e-02, -2.9717e-01,\n",
              "            1.9472e-01, -3.8473e-01,  1.1686e+00, -1.7593e-01,  2.1155e-01],\n",
              "          [ 2.3476e-01, -1.9143e-02, -6.0934e-02,  3.0234e-01, -7.3569e-02,\n",
              "            6.8861e-01,  2.0671e-01, -5.1669e-01, -5.8692e-01, -3.0831e-01],\n",
              "          [-2.2535e-01,  8.1979e-01, -2.7534e-01, -3.1001e-01, -4.8810e-01,\n",
              "            7.3209e-02,  3.4318e-01,  3.2722e-01,  1.4484e-01, -4.5865e-01]])},\n",
              " {'layer1.bias': tensor([-0.1344,  0.0164,  0.0894,  0.1140,  0.1685, -0.0702,  0.1120,  0.2294,\n",
              "           0.2413,  0.0212]),\n",
              "  'layer1.weight': tensor([[ 0.0314, -0.0089, -0.0200,  ..., -0.0010,  0.0217, -0.0199],\n",
              "          [ 0.0315, -0.0011,  0.0030,  ..., -0.0050,  0.0129,  0.0031],\n",
              "          [ 0.0004,  0.0187,  0.0169,  ..., -0.0016,  0.0179,  0.0129],\n",
              "          ...,\n",
              "          [ 0.0140, -0.0001, -0.0761,  ..., -0.0692, -0.0228,  0.0068],\n",
              "          [-0.0102, -0.0163,  0.0129,  ..., -0.0063, -0.0056, -0.0201],\n",
              "          [ 0.0214, -0.0115, -0.0283,  ...,  0.0007, -0.0226,  0.0117]]),\n",
              "  'layer2.bias': tensor([-0.0757,  0.1686, -0.0634, -0.0354,  0.0310,  0.1686,  0.0139,  0.0732,\n",
              "          -0.2383,  0.0133]),\n",
              "  'layer2.weight': tensor([[ 0.7264, -0.1553, -0.1722, -0.5555,  0.5599, -0.1111, -0.6260,  0.1800,\n",
              "           -0.1986, -0.1533],\n",
              "          [-0.6319,  0.1181,  0.6032,  0.7954, -0.1099, -0.1270, -0.3354, -0.3187,\n",
              "            0.1907, -0.2970],\n",
              "          [ 0.4016,  0.0700, -0.0568,  0.3737, -0.1002, -0.5076, -0.1545, -0.0216,\n",
              "           -0.2749,  0.9167],\n",
              "          [-0.2224,  0.2394,  0.3965, -0.4539, -0.0100,  0.4938, -0.2857, -0.4095,\n",
              "            0.3084,  0.7916],\n",
              "          [-0.0998, -0.4111,  0.2982, -0.4149, -0.3762, -0.3263,  1.0014,  0.2301,\n",
              "           -0.1728, -0.2219],\n",
              "          [-0.3140, -0.3787, -0.3798, -0.0549,  0.9844,  0.0644,  0.3092, -0.1931,\n",
              "            0.3125, -0.0373],\n",
              "          [ 0.6535, -0.3617, -0.1014,  0.1863, -0.3434, -0.5851,  0.1677, -0.1899,\n",
              "            0.7004, -0.4577],\n",
              "          [-0.5613, -0.1033, -0.0406,  0.0687, -0.2971,  0.2520, -0.4669,  1.1172,\n",
              "           -0.1753,  0.2447],\n",
              "          [ 0.2745, -0.1130, -0.1787,  0.3679, -0.0251,  0.6382,  0.2814, -0.4793,\n",
              "           -0.6194, -0.2821],\n",
              "          [-0.1718,  0.8275, -0.3243, -0.4315, -0.4142,  0.1142,  0.3046,  0.3434,\n",
              "           -0.0177, -0.4431]])},\n",
              " {'layer1.bias': tensor([-0.0821, -0.0236,  0.0653,  0.1211,  0.2037, -0.0985,  0.1400,  0.2165,\n",
              "           0.1906, -0.0045]),\n",
              "  'layer1.weight': tensor([[ 0.0457, -0.0040, -0.0163,  ..., -0.0117,  0.0109, -0.0305],\n",
              "          [ 0.0332, -0.0012,  0.0035,  ...,  0.0029,  0.0072,  0.0013],\n",
              "          [-0.0009,  0.0082,  0.0117,  ...,  0.0024,  0.0132,  0.0143],\n",
              "          ...,\n",
              "          [ 0.0210,  0.0264, -0.0561,  ..., -0.0312, -0.0086,  0.0708],\n",
              "          [-0.0188,  0.0117,  0.0162,  ...,  0.0017, -0.0241, -0.0049],\n",
              "          [-0.0015, -0.0143, -0.0059,  ...,  0.0353, -0.0302,  0.0511]]),\n",
              "  'layer2.bias': tensor([-0.0795,  0.1516, -0.0825, -0.0531,  0.0181,  0.1880, -0.0169,  0.0773,\n",
              "          -0.2322, -0.0194]),\n",
              "  'layer2.weight': tensor([[ 6.5258e-01,  2.9825e-02, -2.0286e-01, -5.4448e-01,  6.9365e-01,\n",
              "           -1.0240e-01, -6.1354e-01,  1.2442e-01, -2.6136e-01, -1.9868e-01],\n",
              "          [-5.9457e-01,  3.5678e-02,  6.3846e-01,  7.7586e-01, -2.1248e-01,\n",
              "           -2.6853e-01, -2.8483e-01, -2.7330e-01,  2.0501e-01, -2.5848e-01],\n",
              "          [ 4.4020e-01,  6.8426e-02, -2.1944e-01,  3.8501e-01, -8.0741e-02,\n",
              "           -4.4011e-01, -2.4076e-01,  1.5629e-03, -2.8853e-01,  9.1585e-01],\n",
              "          [-2.2493e-01,  2.1902e-01,  4.5685e-01, -4.9273e-01, -1.8207e-02,\n",
              "            4.3144e-01, -3.0249e-01, -2.6585e-01,  3.9732e-01,  7.2462e-01],\n",
              "          [-4.6680e-03, -4.3444e-01,  2.1630e-01, -4.0788e-01, -3.5123e-01,\n",
              "           -3.6082e-01,  1.0318e+00,  1.7210e-01, -1.8188e-01, -2.1423e-01],\n",
              "          [-4.0834e-01, -3.3929e-01, -3.8520e-01, -2.2983e-02,  9.5102e-01,\n",
              "            1.4005e-01,  2.9308e-01, -2.8920e-01,  3.3396e-01, -4.2793e-02],\n",
              "          [ 7.8909e-01, -4.0157e-01, -1.4814e-01,  2.1198e-01, -2.9142e-01,\n",
              "           -4.8960e-01,  1.5138e-02, -1.8323e-01,  6.2776e-01, -4.2569e-01],\n",
              "          [-5.2210e-01, -8.4614e-02,  3.7248e-04,  6.8337e-02, -2.9350e-01,\n",
              "            1.8501e-01, -4.1392e-01,  1.1773e+00, -1.7010e-01,  2.0803e-01],\n",
              "          [ 1.5607e-01, -1.3096e-02, -9.0009e-02,  3.6793e-01, -3.8368e-02,\n",
              "            6.6032e-01,  2.3268e-01, -5.1391e-01, -6.3596e-01, -2.8553e-01],\n",
              "          [-1.6807e-01,  8.4395e-01, -3.1123e-01, -3.7195e-01, -4.2923e-01,\n",
              "            4.6052e-02,  2.9044e-01,  3.1891e-01,  9.2453e-02, -4.3389e-01]])},\n",
              " {'layer1.bias': tensor([-0.1309,  0.0283,  0.0631,  0.0719,  0.1752, -0.0729,  0.1337,  0.2506,\n",
              "           0.1897, -0.0124]),\n",
              "  'layer1.weight': tensor([[ 0.0002, -0.0360, -0.0319,  ...,  0.0050, -0.0046, -0.0286],\n",
              "          [ 0.0327,  0.0038,  0.0010,  ...,  0.0039,  0.0026,  0.0055],\n",
              "          [ 0.0082,  0.0087,  0.0048,  ...,  0.0043,  0.0090,  0.0041],\n",
              "          ...,\n",
              "          [ 0.0121,  0.0279, -0.0485,  ..., -0.0326, -0.0157,  0.0616],\n",
              "          [ 0.0004,  0.0118,  0.0141,  ...,  0.0046, -0.0207,  0.0030],\n",
              "          [-0.0032, -0.0208, -0.0193,  ..., -0.0008, -0.0207,  0.0410]]),\n",
              "  'layer2.bias': tensor([-0.1190,  0.1125, -0.0536, -0.0344, -0.0154,  0.1554,  0.1120,  0.0754,\n",
              "          -0.3062, -0.0451]),\n",
              "  'layer2.weight': tensor([[ 0.7232, -0.1776, -0.2126, -0.5790,  0.5481, -0.0891, -0.6578,  0.1311,\n",
              "           -0.1536, -0.1162],\n",
              "          [-0.6089,  0.2193,  0.5854,  0.8017, -0.0892, -0.1659, -0.3722, -0.3175,\n",
              "            0.1983, -0.3264],\n",
              "          [ 0.4220,  0.0680, -0.0881,  0.4116, -0.1196, -0.5998, -0.1769, -0.0384,\n",
              "           -0.2681,  0.9421],\n",
              "          [-0.1974,  0.2265,  0.4500, -0.4097, -0.0136,  0.4638, -0.3175, -0.3573,\n",
              "            0.3664,  0.7605],\n",
              "          [-0.1050, -0.3931,  0.3601, -0.3995, -0.3811, -0.3540,  1.0435,  0.1954,\n",
              "           -0.1603, -0.2244],\n",
              "          [-0.3752, -0.3574, -0.3777, -0.0502,  0.9556,  0.1505,  0.3249, -0.2606,\n",
              "            0.2895, -0.0581],\n",
              "          [ 0.6512, -0.3699, -0.0423,  0.2406, -0.3433, -0.5279,  0.0465, -0.1791,\n",
              "            0.6878, -0.4378],\n",
              "          [-0.5558, -0.0902,  0.0309,  0.1034, -0.3231,  0.2685, -0.5010,  1.1179,\n",
              "           -0.1344,  0.2447],\n",
              "          [ 0.3042, -0.1775, -0.1887,  0.3640, -0.0087,  0.6363,  0.2390, -0.5174,\n",
              "           -0.6424, -0.2813],\n",
              "          [-0.1539,  0.8483, -0.3299, -0.4212, -0.3737,  0.1028,  0.2349,  0.3079,\n",
              "           -0.0261, -0.4000]])},\n",
              " {'layer1.bias': tensor([-0.1404, -0.0097,  0.1084,  0.0713,  0.1818, -0.1302,  0.1414,  0.2505,\n",
              "           0.1803, -0.0443]),\n",
              "  'layer1.weight': tensor([[ 0.0012,  0.0252, -0.0213,  ...,  0.0166,  0.0480, -0.0375],\n",
              "          [ 0.0380,  0.0047,  0.0006,  ..., -0.0009, -0.0013,  0.0070],\n",
              "          [ 0.0045,  0.0045,  0.0039,  ...,  0.0088,  0.0139,  0.0102],\n",
              "          ...,\n",
              "          [ 0.0202,  0.0467, -0.0309,  ..., -0.0537, -0.0602,  0.0323],\n",
              "          [ 0.0267,  0.0398,  0.0120,  ...,  0.0027, -0.0085,  0.0136],\n",
              "          [-0.0451,  0.0138,  0.0054,  ...,  0.0011,  0.0128,  0.0176]]),\n",
              "  'layer2.bias': tensor([-0.0903,  0.1562, -0.0522, -0.0787,  0.0101,  0.2297,  0.0689,  0.0831,\n",
              "          -0.2650, -0.0640]),\n",
              "  'layer2.weight': tensor([[ 0.6944, -0.0635, -0.2421, -0.5723,  0.6054, -0.0283, -0.6446,  0.0733,\n",
              "           -0.2499, -0.1356],\n",
              "          [-0.5829,  0.0526,  0.6501,  0.7667, -0.2346, -0.3149, -0.2576, -0.2882,\n",
              "            0.2565, -0.2094],\n",
              "          [ 0.3823,  0.0525, -0.1605,  0.3907, -0.0917, -0.4150, -0.2092, -0.0654,\n",
              "           -0.2792,  0.9744],\n",
              "          [-0.1920,  0.2016,  0.5438, -0.4838, -0.0071,  0.4579, -0.3061, -0.2980,\n",
              "            0.3653,  0.7398],\n",
              "          [-0.0336, -0.4326,  0.2102, -0.4295, -0.3445, -0.3590,  1.0254,  0.1606,\n",
              "           -0.2042, -0.2074],\n",
              "          [-0.4147, -0.3144, -0.4164, -0.0233,  0.8942,  0.1501,  0.3133, -0.3340,\n",
              "            0.3604, -0.0812],\n",
              "          [ 0.7857, -0.4255, -0.1723,  0.1925, -0.3303, -0.4596, -0.0556, -0.1739,\n",
              "            0.5754, -0.4457],\n",
              "          [-0.5605, -0.0683,  0.0440,  0.0803, -0.2757,  0.1273, -0.4541,  1.1316,\n",
              "           -0.1706,  0.1908],\n",
              "          [ 0.2224, -0.0229, -0.1399,  0.3790, -0.0868,  0.7313,  0.2392, -0.4915,\n",
              "           -0.6088, -0.3120],\n",
              "          [-0.1205,  0.8424, -0.2886, -0.3531, -0.4744,  0.0555,  0.2763,  0.2766,\n",
              "            0.1011, -0.4016]])}]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_mean_dict = post_mean(fed_mod.saved_weights)"
      ],
      "metadata": {
        "id": "9yz1nB4K1gQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(fed_mod.central_model['model'], './model_no_processing')"
      ],
      "metadata": {
        "id": "rlVECc061gcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fed_mod.central_model['model'].load_state_dict(post_mean_dict)"
      ],
      "metadata": {
        "id": "cUN-QiS71gmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8492b32b-ccaf-49e1-a984-8d1cd7cd82f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses, test_accuracies, test_overall_accuracy\\\n",
        ", raw_test_output, raw_test_labels = Test(fed_mod, data_test, device)\n",
        "print(test_overall_accuracy)"
      ],
      "metadata": {
        "id": "W7BJTj9d1_wr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830dd92a-0028-46a6-a60b-bd24a48a5196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9119)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(test_losses)"
      ],
      "metadata": {
        "id": "V_tK9yTPtkxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ff4e29-63be-4525-9e43-21e1c85ea518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3672176763415337"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calibration"
      ],
      "metadata": {
        "id": "5OqhB9Rc5hev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reliability(outputs, targets, m_bins):\n",
        "  boundaries = [i*1/m_bins for i in range(1,1+m_bins)]\n",
        "  accuracy = [0 for e in range(m_bins)]\n",
        "  conf = [0 for e in range(m_bins)]\n",
        "  totals = [0 for e in range(m_bins)]\n",
        "\n",
        "  \n",
        "  y, y_ind = F.softmax(outputs, dim = 1).max(dim = 1)\n",
        "\n",
        "  correct = 1.0*(y_ind == targets)\n",
        "  for i, y_i in enumerate(y):\n",
        "    for j, edge in enumerate(boundaries):\n",
        "      if y[i] <= edge:\n",
        "        totals[j] += 1\n",
        "        accuracy[j] += correct[i].item()\n",
        "        conf[j] += y_i.item()\n",
        "        break\n",
        "\n",
        "  for i in range(len(totals)):\n",
        "    if totals[i] != 0:\n",
        "      accuracy[i] /= totals[i]\n",
        "      conf[i] /= totals[i]\n",
        "\n",
        "  \n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_axes([0, 0, 1, 1])\n",
        "  edges = [1/m_bins*i for i in range(m_bins)]\n",
        "  accuracy = np.array(accuracy)\n",
        "  conf = np.array(conf)\n",
        "\n",
        "  diff = conf - accuracy\n",
        "  base = accuracy\n",
        "\n",
        "  ax.bar(edges, base, align = 'edge',\n",
        "         width = 1/m_bins, alpha = 1, color = '#2c6fbb', edgecolor = 'black',\n",
        "         linewidth = 0.1, label = \"outputs\")\n",
        "  \n",
        "  ax.bar(edges, diff, bottom = base, align = 'edge', width = 1/m_bins, color = \"red\",\n",
        "         edgecolor= \"red\", alpha = .5, linewidth = 1, hatch = \"/\", label = \"gap\")\n",
        "  \n",
        "  ax.plot([0,1],[0, 1], \"--\", alpha = 0.6, linewidth = 3, c = \"black\")\n",
        "\n",
        "  ax.legend()\n",
        "\n",
        "  ECE = 1.0/sum(totals)*sum(totals[i]*abs(accuracy[i]-conf[i])\\\n",
        "                                  for i in range(m_bins))\n",
        "  \n",
        "  ax.set_title('Reliability Diagram with ECE {:.3f}'.format(ECE))\n",
        "  plt.plot()\n",
        "\n",
        "  return accuracy, conf, totals, ECE"
      ],
      "metadata": {
        "id": "rlpcm4b15oEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_bins = len(MNIST.classes)\n",
        "acc, conf, total, ECE = plot_reliability(raw_test_output, raw_test_labels, m_bins)"
      ],
      "metadata": {
        "id": "XeUVXLBf5oJl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "6e1dccbc-20e2-42e6-afdb-fb1f69bbbb86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Out of Sample Test"
      ],
      "metadata": {
        "id": "6uXGsCI-yLOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses_oos, test_accuracies_oos, test_overall_accuracy_oos\\\n",
        ", raw_test_output_oos, raw_test_labels_oos = Test(fed_mod, data_test_oos, device)\n",
        "print(test_overall_accuracy_oos)"
      ],
      "metadata": {
        "id": "w8rtTbELyFhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41a5062-0d1d-42d5-d298-97f94dc05cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0965)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc, conf, total, ECE = plot_reliability(raw_test_output_oos, raw_test_labels_oos, m_bins)"
      ],
      "metadata": {
        "id": "ZZy62gxPyVxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "432c3f26-8b59-462b-f4ab-deaa1dfb423b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}